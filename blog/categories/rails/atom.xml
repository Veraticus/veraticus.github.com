<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rails | Josh Symonds]]></title>
  <link href="http://joshsymonds.com/blog/categories/rails/atom.xml" rel="self"/>
  <link href="http://joshsymonds.com/"/>
  <updated>2012-06-01T10:59:43-05:00</updated>
  <id>http://joshsymonds.com/</id>
  <author>
    <name><![CDATA[Josh Symonds]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Dynamic Routing with Rails]]></title>
    <link href="http://joshsymonds.com/blog/2012/05/22/dynamic-routing-with-rails/"/>
    <updated>2012-05-22T23:08:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/05/22/dynamic-routing-with-rails</id>
    <content type="html"><![CDATA[<p>I love Rails' routing system. Quickly and easily connecting English-readable URLs to complicated web actions is one of the joys of working in one of the coolest web frameworks on the Internet. At Synthetic, we're ramping up to get a new site out, and as part of the push for that I implemented some cool dynamic routing. I wanted to briefly summarize how and why I went with the routing choices I did, to illustrate using routes expressively and, hopefully, easily.</p>

<!-- more -->


<h2>The Problem</h2>

<p>Hipstamatic has gear. Lots and lots of gear. We have over 100 lenses, flashes, and films; but from a programmatic perspective they're all really similar -- they all have a name, a description, some assets associated to them... And so they're all lumped together in a single model called <code>Gear</code>. Our new application is intended to let users browse our entire gear collection, and so obviously it has a single controller that (surprise surprise) allows you to see each piece of gear. Thus we wind up with routes like this:</p>

<p><code>
  gear/alfred
  gear/dreampop
  gear/stache
</code></p>

<p>And that's kind of tragic. Each type of gear should have its own route, even if they're all in the same model. Something like this would be both more readable and more expressive:</p>

<p><code>
  lenses/alfred
  flashes/dreampop
  films/stache
</code></p>

<p>Of course, having a controller for each kind of gear would be crazy. Happily, Rails provides some easy routing solutions!</p>

<h2>The Solution</h2>

<p>In our routes.rb, for each kind of resource that we're expecting, we create matching routes:</p>

<p><code>ruby
[:lenses, :films, :flashes, :cases].each do |gear|
  match "#{gear}/:id", :controller =&gt; :gear, :action =&gt; :show, :type =&gt; gear, :as =&gt; gear
end
</code></p>

<p>Let's dissect this routing statement in two steps. For the first, obviously, we're linking a route like <code>lenses/a1</code> or <code>flashes/cherry</code> to the gear controller. Importantly we're also passing a type: either lenses, films, flashes, or cases, instructing the controller which endpoint we want. In said controller, we should make sure that this passed type and the name of the piece of gear match. Otherwise people would go to <code>lenses/dreampop</code> or <code>flashes/alfred</code> and still see the correct resource despite specifying an invalid type and name combination. That would just be madness!</p>

<p>```ruby
class GearController &lt; ApplicationController
  before_filter :find_gear</p>

<p>  private</p>

<p>  def find_gear</p>

<pre><code>@gear = Gear.type(params[:type].to_s.singularize).where(:reference =&gt; params[:id]).first if params[:id]
</code></pre>

<p>  end
end
```</p>

<p>Type is a named scope that matches the passed params[:type] to a database column storing the actual type of gear. So this will try to find a lens or flash or what have you with the appropriate reference name. Simple enough.</p>

<p>For the second part of the above route, the <code>:as =&gt; gear</code> part creates named routes like lenses_path and films_url. With that in mind, it's easy for us to dynamically generate links based only on the gear object. This is the helper that allows us to do so:</p>

<p><code>ruby
def multi_path(obj)
  self.send("#{obj.type.pluralize.downcase}_path".to_sym, obj.reference)
end
</code></p>

<p>So I can use <code>multi_path(Gear.find_by_name('Kaimal Mark II'))</code> and the route will be generated as if I had typed <code>lenses_path</code> instead of <code>multi_path</code>: similarly with any object that responds to type and reference, which happily for me is all gear. This is important because I don't want to have to use a switch in iterated blocks to figure out which path name I want to use. Now I can just use multi_path and be guaranteed that the correct one will be selected.</p>

<p>With only a few lines of code, it's easy to make routes that were previously clunky and unexpressive into sensible, readable endpoints. And you don't have to sacrifice DRY to do so. If you have a lot of data that's expressed through only one controller, consider dynamic routing like this. It's easier for customers to remember URLs that are readable to them, and this is a good way to make that happen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quick &amp; Easy User Preferences in Rails]]></title>
    <link href="http://joshsymonds.com/blog/2012/05/16/quick-and-easy-user-preferences-in-rails/"/>
    <updated>2012-05-16T18:11:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/05/16/quick-and-easy-user-preferences-in-rails</id>
    <content type="html"><![CDATA[<p>My first RubyMotion application is rapidly nearing completion. As it involves user preferences that have to be stored both locally and remotely, I was investigating the available Rails gems for user preferences and really didn't like what was presently out there. I don't really have time to maintain another gem, but maybe someone else has run into this problem and wants a quick and easy solution for creating user preferences. If so, then this code's for you.</p>

<!-- more -->


<h2>Setting Up Preferences</h2>

<p>You need a preference model. It should look like this:</p>

<p>```ruby
class Preference &lt; ActiveRecord::Base
  belongs_to :user</p>

<p>  validates_uniqueness_of :name, :scope => :user_id</p>

<p>  attr_accessible :name, :value
end
```</p>

<p>I used this migration to create it:</p>

<p>```ruby
class CreatePreferences &lt; ActiveRecord::Migration
  def change</p>

<pre><code>create_table :preferences do |t|
  t.string :name, :value
  t.integer :user_id
  t.timestamps
end
</code></pre>

<p>  end
end
```</p>

<p>You might note that the value of all preferences, regardless of if they're supposed to be Boolean or datetime, is a string. Keep this in mind when you have to query this field later. (That is, if you want to search for all preferences where the value is true, you'll want to search for "1". And similarly, doing <code>user.preferences.first.true?</code> will always return <code>true</code>, as any string value is true. So, coder beware!)</p>

<h2>Using Them</h2>

<p>Ideally, this is what we want the user model to look like.</p>

<p>```ruby
class User &lt; ActiveRecord::Base
  include Preferences</p>

<p>  preference :chime, false
  preference :name, "Josh"
  preference :awesome, true</p>

<p>  ...
end
```</p>

<p>Simple but straightforward: we include the module and then define each preference, with its name first and default value second. Ideally we don't want to save default values to the database, since that would just make a lot of unnecessary records.</p>

<h2>The Preferences Module</h2>

<p>So let's make that happen in that <code>include Preferences</code> line! This is the real heart of the preferences engine.</p>

<p>```ruby
module Preferences
  extend ActiveSupport::Concern</p>

<p>  included do</p>

<pre><code>has_many :preferences
@@preferences = {}
</code></pre>

<p>  end</p>

<p>  module ClassMethods</p>

<pre><code>def preference(name, default)
  preferences = self.class_variable_get(:'@@preferences')
  preferences[name] = default
  self.class_variable_set(:'@@preferences', preferences)
end
</code></pre>

<p>  end</p>

<p>  def read_preference(name)</p>

<pre><code>if p = self.preferences.where(:name =&gt; name).first
  return p
end
return self.preferences.new(:name =&gt; name, :value =&gt; @@preferences[name]) if @@preferences.has_key?(name)
nil
</code></pre>

<p>  end</p>

<p>  def write_preference(name, value)</p>

<pre><code>p = self.preferences.find_or_create_by_name(name)
p.update_attribute(:value, value)
</code></pre>

<p>  end</p>

<p>  def method_missing(method, *args)</p>

<pre><code>if @@preferences.keys.any?{|k| method =~ /#{k}/}
  if method =~ /=/
    self.write_preference(method.gsub('=', ''), *args)
  else
    self.read_preference(method)
  end
else
  super
end
</code></pre>

<p>  end
end
```</p>

<p>This is really pretty simple. Upon inclusion it tells the model that it's a part of that it <code>has_many :preferences</code> and sets up a class variable hash to store preferences and their defaults. When you declare <code>preference :chime, true</code> it records that in the class variable, and then all instances will respond to either <code>user.chime = true</code> or <code>user.write_preference(:chime, true)</code>. You can read values with <code>user.chime</code> or <code>user.read_preference(:chime)</code>. If a value isn't written in the database, it returns the default value instead.</p>

<p>This probably has a level or two of refactoring that could happen around it. Maybe when I have time I will turn it into a more sensible gem, but until then, if anyone needs quick and dirty preferences... here you go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Elasticsearch in Rails with Tire]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/15/testing-elasticsearch-in-rails-with-tire/"/>
    <updated>2012-04-15T23:38:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/15/testing-elasticsearch-in-rails-with-tire</id>
    <content type="html"><![CDATA[<p>In my <a href="http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails/">previous entry on elasticsearch</a>, I promised I would elaborate on testing <a href="http://www.elasticsearch.org/">elasticsearch</a> (and <a href="https://github.com/karmi/tire">tire</a>) in Rails applications. There's not really a whole lot of secret sauce to it, but I figured it'd make a good, quick post with some crunchy code for a late night. While writing, though, I realized I could also talk about a small problem I ran into while using tire -- specifically relating to index regeneration. This isn't a major flaw, but it did waste some of my time, so I figured documenting it (prior to fixing it) would be a sensible idea.</p>

<!-- more -->


<h2>Testing Tire</h2>

<p>There are two components to testing tire: the first is emptying the index before tests where the contents of the index matters, and the second is ensuring that you only delete the index you want, rather than your development index (which would be annoying). Deleting the correct index is really easy. You just want something like this in your model:</p>

<p>```ruby
class Photo
  include Tire::Model::Search</p>

<p>  index_name("#{Rails.env}-search-photos")</p>

<p>  ...
end
```</p>

<p>Specifying <code>index_name</code> as dependent on the Rails environment ensures that your development index won't be destroyed by the next bit of code.</p>

<p><code>ruby
def clear_photo_index
  Photo.tire.index.delete
  Photo.tire.index.create(:mappings =&gt; Photo.tire.mapping_to_hash, :settings =&gt; Photo.tire.settings)
  Photo.tire.index.refresh
end
</code></p>

<p>I stuck that code in <code>test_helper.rb</code> and I call it before each of my photo tests. The first line, obviously, deletes the entire index. The second recreates it, using the mappings and settings already specified in the Photo model. And then we refresh it just to make sure that tire agrees with elasticsearch about the indexed fields.</p>

<h2>Caveat Indexor</h2>

<p>Overall, tire and elasticsearch have been joys to use. I have experienced unexpected behavior in tire though, particularly relating to index mappings. Obviously, deleting an index in tire works just as expected -- the index and all its associated data goes away. Also deleted are the field mappings for that index. However, what happens when you try to create a new object without reloading the class that defined it?</p>

<p>Tire still faithfully stores the object into the deleted index. This invokes elasticsearch's <a href="http://www.elasticsearch.org/guide/reference/api/index_.html">automatic index creation</a> logic, which attempts to determine the types of your fields manually. Unfortunately, it never seems to correctly identify geo_point fields properly. For example, this is what my index mapping should look like:</p>

<p><code>ruby
{"photo"=&gt;{"properties"=&gt;{"account_id"=&gt;{"type"=&gt;"string"}, "id"=&gt;{"type"=&gt;"string"}, "lat_lng"=&gt;{"type"=&gt;"geo_point"}, "name"=&gt;{"type"=&gt;"string", "analyzer"=&gt;"snowball"}}}}
</code></p>

<p>But if I delete the index and then insert an object into it, elasticsearch automatically determines the types as follows:</p>

<p><code>ruby
{"photo"=&gt;{"properties"=&gt;{"_type"=&gt;{"type"=&gt;"string"}, "account_id"=&gt;{"type"=&gt;"long"}, "id"=&gt;{"type"=&gt;"long"}, "lat_lng"=&gt;{"type"=&gt;"string"}, "name"=&gt;{"type"=&gt;"string"}}}}
</code></p>

<p>The key difference here is that <code>lat_lng</code> is not a geo_point but is instead a string, which prevents any of the index geolocation queries from being run on it. You can correct this problem by deleting the index and reloading the class in which the index is defined, which causes tire to create the index again from your provided mapping. (Or run the <code>tire.index.create</code> code from above.) But I spent a tiring(pun!) hour trying to figure out why my indexes kept on receiving inappropriate field types before hitting on this as the reason.</p>

<p>Similarly, and possibly more frustratingly, if you are incrementally developing an index, changes to your mapping won't appear in the index until you delete said index and reload its defining class. Again, deleting the index and inserting data immediately will cause elasticsearch to guess the field mappings for your index, with tragically inconsistent results.</p>

<p>I told the very talented <a href="https://github.com/karmi">karmi</a> about this problem and he sensibly suggested I write a failing test for it, though unfortunately I haven't had the time to sit down and really do that. In the meantime, just know that this annoyance exists, and if you're working on tire indexes, make sure you religiously delete the mapping and then reload the class before you attempt to use the index again.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reducing Our AWS Costs by 60%]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/12/reducing-our-aws-costs-by-60-percent/"/>
    <updated>2012-04-12T13:30:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/12/reducing-our-aws-costs-by-60-percent</id>
    <content type="html"><![CDATA[<p>Hipstamatic's Rails application is deployed to Amazon's Elastic Cloud, and we make extensive use of Amazon's Web Services in keeping it nimble and performant. Last month, I dedicated two weeks to increasing the responsiveness of the application while simultaneously improving its performance. As a result of the changes I implemented, our AWS costs for this month will be 60% lower than they were last month. This is a pretty dramatic drop, and I wanted to discuss the tools and techniques I used to make it happen.</p>

<!-- more -->


<h2>Improve Database Performance</h2>

<p>One of the biggest cost savings I implemented was scaling back our RDS instance. We use a multi-AZ deployment to ensure constant availability; unfortunately, multi-AZ instances are extremely expensive, and I knew that if we could decrease the size of our instance we could save quite a bit of money. Targeting this part of our infrastructure proved very fruitful for lowering costs, and here's what I did to decrease load on our database:</p>

<h3>Find Slow Queries</h3>

<p>Step through everything your app does with <a href="https://github.com/flyerhzm/bullet">bullet</a> and Rails 3.2's <a href="http://weblog.rubyonrails.org/2011/12/6/what-s-new-in-edge-rails-explain/">slow query explainer</a>. Really get into the nitty-gritty here: run your resque jobs, go to every single controller action, and run all the model code you can get your hands on. Make sure you have a lot of data loaded into your database when you do this, or else queries that might be slow won't show up since they aren't running under actual load conditions. I added hundreds of thousands of records using FactoryGirl like this (from the Rails console):</p>

<p><code>ruby
require 'factory_girl'
require '.test/factories.rb' # or wherever your factory file is located
200000.times do
  Factory.create(:photo)
end
</code></p>

<p>Repeat as needed. Remember your production system will likely have millions of records; get as close as you can to this without overwhelming your development machine.</p>

<h3>Index Lots</h3>

<p>Index the crap out of everything. It's nearly impossible to have too many indexes in a relational database, and every field you execute a query on should be indexed. Usually under production load conditions you'll rapidly discover that various fields that aren't indexed together should be, but examining the individual queries your application makes and ensuring they're all indexed is an awesome (though boring) use of your time.</p>

<p>I used <a href="https://github.com/plentz/lol_dba">lol_dba</a> to get a starting point for indexes I wanted to add, but honestly you'll have to get into your code yourself to find out what really needs indexing. Automated tools can't really replace actual hands-on experience... at least, not in this case.</p>

<h3>Don't Use Your Database</h3>

<p>Sticking something into the database isn't always the right solution to a problem. I discussed <a href="http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails/">in a previous post</a> the problems we encountered in implementing a database-driven solution for something that should never have touched the database. Extremely large join tables in particular have awful performance, and the indexes on them can rapidly grow to a really ridiculous size. Before putting something in the database, consider if there isn't another tool to do that job. In particular, think about:</p>

<ul>
<li><h4>memcached</h4>

<p>If you want to rapidly retrieve data and it's in key-value form, and persistence doesn't really matter, use memcached instead. Be severe when you consider if something needs persistence. Do you really need to keep <em>every</em> message you pass to a client, or would keeping a count of them be sufficient?</p></li>
<li><h4>redis</h4>

<p>Redis gives you the benefits of a semi-persistent datastore with some really nice data structures. If you need lists, sets, or ordered sets -- especially if these data structures are going to end up being extremely large or called very frequently -- use redis.</p></li>
<li><h4>elasticsearch</h4>

<p>For geospatial, filter-based, and/or full-text indexing, relational database performance has nothing on dedicated indexing tools. I can't say enough nice things about elasticsearch in general and <a href="https://github.com/karmi/tire">tire</a> in particular. elasticsearch is easy to set up, has a fraction of the overhead of a database, and several times its speed. If you're performing a complicated, variated SQL query, consider if that query could be run on an indexing engine instead.</p></li>
</ul>


<h2>Use More Caching</h2>

<p>I touched on this in <a href="http://joshsymonds.com/blog/2012/04/06/how-i-scaled-hipstamatic/">my scaling post</a> but I want to restate it here: caching allows you to reduce load on every part of your application. (Except the cache I guess...) With proper caching you can remove web servers, application servers, and database servers from your setup. In addition to scaling down our database instance, we removed two entire extra-large EC2 instances because of better caching.</p>

<p>Figure out what to cache first and foremost by investigating your metrics. New Relic, Google analytics, even munin and monit will all provide you clues as to where users are going. I'd be willing to bet money almost all of your traffic is directed to the same five or ten extremely popular sources. Extract partials from those pages, or just cache them in their entirety: then serve the results instead of hitting your database (or ideally even before hitting your application servers).</p>

<p>The most important key to our caching strategy are definitely Rails sweepers. Rails sweepers keep caching DRY: instead of expiring caches manually over and over in your models and controllers, do it in one centralized place. Just keep in mind that sometimes Rails' helper methods won't find the proper cache, especially if you use multiple domains for one application. In that case specify the fragment you need to expire directly, like so:</p>

<p>```ruby
def after_save(object)
  [:domain1, :domain2, :domain3].each do |domain|</p>

<pre><code>expire_fragment "views/#{domain}/objects/#{object.id}"
</code></pre>

<p>  end
end
```</p>

<p>I have this extracted out on a per-environment basis, specifying the domains to expire in our environment files. It works out really well.</p>

<h2>Be Responsive</h2>

<p>One of the great things about being deployed to the cloud is that you can -- and should! -- scale up and down frequently. I usually have fifteen stopped EC2 instances sitting around, waiting to be added to my stack: with only a quick bootup, they'll be available to handle web or application traffic, or even add more resque workers and extra redis or memcached instances. These tools are highly dynamic and most of them can be scaled up and down quite easily, and stopped EC2 instances cost you nearly nothing (as long as the provisioned EBS drive is relatively small). And their cost is really very affordable when you consider how agile they allow you to be.</p>

<p>The key to being responsive is to communicate with your business. Is there a press release dropping that day? Time to scale up. New product coming soon? Bring those application servers online. Monitor your metrics carefully when you're at heightened capacity; when it looks like traffic has slowed, feel free to scale back down. But always be wary -- getting featured in a big publication can crush your servers unexpectedly. Ensure that you either have automated tools or great alerts letting you know when you're getting hammered, and don't be afraid to bring more servers online in a hurry. Overscaling in the short-term is a great idea: even the biggest EC2 instances cost only a few dollars an hour, and the peace of mind they give you is priceless.</p>

<p>Using these methods me allowed me to streamline our stack really significantly. Not only are we faster than we were just three months ago; we're saving a boatload of money each month. And being more awesome while spending less money is definitely a win/win.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Achieving 100% Uptime]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime/"/>
    <updated>2012-04-09T17:52:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime</id>
    <content type="html"><![CDATA[<p><img src="http://f.cl.ly/items/0q2M3B2o1f1D1D451B1S/uptime.jpg" alt="Uptime - 100%" /></p>

<p>Keeping a highly available web application online is no joke. Everything above 99% is extremely impressive; that means that you battled the forces of <a href="http://en.wikipedia.org/wiki/Software_erosion">erosion</a> and probably even deployed some pretty neat features without even a hiccup from your users' perspective. I always feel great when I get our weekly <a href="http://newrelic.com/">New Relic</a> status report email -- it's a good indication of how well I did my job in the previous week. And for a couple weeks now I'm happy to report I've been very proud indeed, with 100% uptime on the Hipstamatic web application.</p>

<p>How do you achieve numbers like these? Unfortunately getting to 100% isn't an easy road, and I want to state up front that I also don't think it's a realistic goal. Issues you can't control can ruin your uptime number, and you shouldn't feel broken up about that. It happens to everybody. But it's always good setting goals that are difficult to achieve, and this one is no different.</p>

<p>So what's the secret to 100% uptime?</p>

<!-- more -->


<h2>Watch It Constantly</h2>

<p>Some people check their fantasy baseball league or their portfolio every morning. At the slightest hint of trouble, they'll be waist-deep in trading players or stocks to get everything right back on track. You should be that way with your servers and the software that runs on them. This usually means monitoring software, and a lot of it.</p>

<p>At Hipstamatic, we make extensive use of New Relic to give us a broad overview of our application. It helps us proactively fix nascent problems, analyzing slow queries and sluggish pages. But you need something closer to the metal, and for that we use <a href="http://mmonit.com/monit/">monit</a>. Monit is an amazing tool to control your applications' behavior and warn you when that behavior becomes dangerous. Here's a sample of our unicorn monit file:</p>

<p><code>bash
  if totalmem &gt; 70% for 4 cycles then alert
  if totalmem &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
  if cpu &gt; 70% for 4 cycles then alert
  if cpu &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
</code></p>

<p>This states that I get an alert when unicorn's total memory or CPU usage exceeds 70%, and that unicorn receives a USR2 signal when total memory or CPU exceed 90%.</p>

<p>Finally, we employ <a href="http://munin-monitoring.org/">munin</a> to compile statistics that we care about, including nginx connections and unicorn requests served.</p>

<p>Yes, this is a lot of monitoring. But I feel like even this isn't enough. You can't watch your stack too carefully, and you can't have too many tools in place to help you analyze what's going on. Consider this trifecta of tools only a start, but at least it's a good one.</p>

<h2>Seamless Deploys</h2>

<p>On an average week I deploy seven to ten times. Of course, this entire process is invisible to our users; the magic that makes this happen is <a href="http://unicorn.bogomips.org/">unicorn</a>. There have been many posts on the wonders of unicorn and how to configure it correctly. I will simply post the part of our <code>unicorn.rb</code> that allows us to do seamless restarting, which you can find in a number of gists essentially unmodified.</p>

<p>```ruby
before_fork do |server, worker|
  old_pid = "#{server.config[:pid]}.oldbin"
  if File.exists?(old_pid) &amp;&amp; server.pid != old_pid</p>

<pre><code>begin
  Process.kill("QUIT", File.read(old_pid).to_i)
rescue Errno::ENOENT, Errno::ESRCH
end
</code></pre>

<p>  end
end
```</p>

<p>The command we use to restart unicorn is:</p>

<p><code>bash
  if [ ! -f '/pids/unicorn.pid' ]; then cd current_path &amp;&amp; bundle exec unicorn_rails -c ./config/unicorn.rb -E production -D; else kill -USR2 `cat /pids/unicorn.pid`; fi
</code></p>

<p>USR2 is the signal that tells unicorn to start reloading itself: the before_fork causes the new server to kill the old server only when it's ready to start processing connections.</p>

<h2>Migrations Without Downtime</h2>

<p>The last key component to 100% uptime is migrating your database without bringing your site down. Of course, this only applies if you're changing how existing code interacts with the database -- for new tables, simply migrate before deploying and you're done. If only it could be that easy all the time...</p>

<p>Frequently we are required to change existing tables and colums or add new ones. For those of us still using relational databases, migrations almost always mean locked tables, and locked tables mean site downtime. To fix this problem, my tool of choice has been <a href="https://github.com/soundcloud/large-hadron-migrator">Large Hadron Migrator</a>. Large Hadron Migrator requires very little from your tables (just an autoincrementing ID) and allows you to alter tables and even add new columns without bringing your site down.</p>

<p>```ruby
class AddOrdersCountToUsers &lt; ActiveRecord::Migration</p>

<p>  def self.up</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} ADD COLUMN orders_count INT(11) default 0")
end
</code></pre>

<p>  end</p>

<p>  def self.down</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} DROP COLUMN orders_count")
end
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>Yes, unfortunately, this includes raw SQL. There is a <a href="http://rubydoc.info/github/soundcloud/large-hadron-migrator/master/Lhm/Migrator#add_column-instance_method">small DSL</a> that exposes a few common methods, but for anything really deep you're gonna need to get your hands dirty. Using this method you'll be able to become the envy of your friends and peers, for you'll be able to execute zero downtime migrations.</p>

<p>And those three points are the main ways I've reduced our downtime. It's a difficult road to 100%, but it's worth it because you can stare at pretty graphs like this:</p>

<p><img src="http://f.cl.ly/items/470B350J0U0q1u3r0T0s/availability-1.jpg" alt="Better than Facebook" /></p>

<p>And imagine that your website and a 100% bar are sitting right at the very tippy top.</p>
]]></content>
  </entry>
  
</feed>
