<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rails | Josh Symonds]]></title>
  <link href="http://joshsymonds.com/blog/categories/rails/atom.xml" rel="self"/>
  <link href="http://joshsymonds.com/"/>
  <updated>2012-04-04T11:42:46-05:00</updated>
  <id>http://joshsymonds.com/</id>
  <author>
    <name><![CDATA[Josh Symonds]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Refactor a Large and Old Project]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/03/how-to-refactor-a-large-and-old-project/"/>
    <updated>2012-04-03T10:06:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/03/how-to-refactor-a-large-and-old-project</id>
    <content type="html"><![CDATA[<p>The Rails application backing Hipstamatic is very, very large. It started over two years ago as a Rails 2.1 project, and has been continuously improved since then -- moving to Rails 3.2, adding in redis and resque, and then adding in elasticsearch. During that time the database has bounced around continuously in size and importance as we move data from MySQL to data stores that are <a href="http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails/">better suited for it</a>. And while at the start it handled only contests and submissions, since then we've added in orders, the family album, D-Series support, and even more exciting behind-the-scenes stuff.</p>

<p>And as you can imagine from a project that's undergone continuous improvement for a long time, it's kind of a mess. A lot of stuff was done without an eye towards our future needs, and, even more embarrassingly, a lot of stuff was done with a future need in mind -- and, of course, that need never materialized, so the code is named or structured improperly.</p>

<p>The temptation with any project as large and old as this is to do <a href="http://chadfowler.com/2006/12/27/the-big-rewrite">the Big Rewrite</a>. I've been involved in a few of those, and my advice regarding them is quite simple:</p>

<!-- more -->


<h1>DON'T DO IT</h1>

<p>There are <a href="http://mentalized.net/journal/2010/10/04/avoiding_the_big_rewrite/">multiple</a> <a href="http://www.joelonsoftware.com/articles/fog0000000069.html">articles</a> <a href="http://blog.objectmentor.com/articles/2009/01/09/the-big-redesign-in-the-sky">discussing</a> why the Big Rewrite is a horrible idea, and reiterating their excellent points here isn't my intention. Instead I'm going to discuss the plan I've decided on for our own project, which I'm going to call, in lieu of the Big Rewrite, the Proper Refactoring.</p>

<p>Our Proper Refactoring will split our one monolithic application apart into a number of services, exposing their APIs for the happiness of our users while hiding the internals of their business logic from parts of the application that don't care about it. If you want to follow along on our process (or do your own Proper Refactoring), I wrote down a quick summary of the three (well, four) simple stages that will take us from having one large working app to many small working apps!</p>

<ol>
<li><h3>Test it all first.</h3>

<p>This entire process is doomed to failure if your application isn't tested. There's no way you can achieve 100% test coverage, regardless of what <a href="https://github.com/colszowka/simplecov">SimpleCov</a> tells you -- there's always that quick fix you stuck in to fix a small problem that isn't tested and won't show up in any coverage report. But you need to get as close as humanly possible, because stuff will break (like that quick fix), and you can limit how much breakage occurs by testing everything you can before you start.</p>

<p>Happily, Hipstamatic is well tested, so step zero for us is pretty well completed. I still anticipate problems will occur as we make the change, and of course as I find code that isn't adequately tested I'll write tests for it... but both of those are unavoidable.</p></li>
<li><h3>Find breakpoints and map splits.</h3>

<p>Hipstamatic will be turning into five services: authentication, photos, contests, ordering, and D-Series cameras. Our main goal is to silo concerns apart from each other, making each part of the application more failure resistant and robust while allowing us to develop them all independently from each other if necessary. I'm not 100% settled on this separation of concerns, but the order that I listed them is the order I'll be working on them. If it seems like something just has to be married to something else, I'll combine them together and that'll be that.</p>

<p>So if you're doing this on your own project, split your application into units that are atomic enough that they can be changed independently from each other, but not so atomic that close couplings are undone. My benchmark for this is going to be if I have two projects open simultaneously and keep coding in the two of them in tandem, most likely they should be merged.</p></li>
<li><h3>Start copying and pasting.</h3>

<p>The fun part! Take the parts necessary for the one fragment you're working on and merge them into one coherent project. Crucically, <em>don't change anything except what's absolutely necessary</em>. You'll find code that you want to change, trust me. Just slap some TODOs on that baby and keep moving. It's important that you change as little as possible, because the process is already breaking apart your nice pretty app. If you start changing the pieces once they're broken, you'll find they don't fit back together quite right, and that will be an enormous headache to fix.</p></li>
<li><h3>Add relevant bits to the API Gem.</h3>

<p>For our web services to understand each other, and to prevent duplication of code, I'll be extracting connector bits into a Gem that each application (and indeed any application that wants to consume our API) can use. It'll most likely be heavily based on <a href="https://github.com/jnunemaker/httparty">httparty</a> since ActiveResource isn't anywhere near as actively developed.</p></li>
</ol>


<p>And the split is complete! Of course it sounds pretty easy when you gloss over most of the hard work in step #2, but hey, the way to make complicated projects seem achievable is to reduce them into manageable steps. I intend to follow this road map like the Pope follows the Bible -- that is, using the good parts and ignoring the rest. Zing! But I'll report back in a future post to indicate how well these steps worked for me. Until then, wish me luck!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Delegation when Delegate Just Won't Work]]></title>
    <link href="http://joshsymonds.com/blog/2012/03/28/delegation-when-delegate-just-wont-work/"/>
    <updated>2012-03-28T15:37:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/03/28/delegation-when-delegate-just-wont-work</id>
    <content type="html"><![CDATA[<p>Rails provides a really awesome ability to avoid <a href="http://en.wikipedia.org/wiki/Law_of_Demeter">Law of Demeter</a> violations -- the <a href="http://apidock.com/rails/Module/delegate">Module#delegate</a> method. The Law of Demeter is an informal programming guideline, intended to make your code more obvious and more reusable: objects should only call methods on other objects, not objects of those objects. To provide a more concrete example:</p>

<p><code>ruby
  @user.address.street_address # Law of Demeter violation: @user should not reach into address!
  @user.street_address # So much better...
</code></p>

<p>Rails, because it's cool, provides a quick and easy pattern for making this work:</p>

<p>```ruby
class User &lt; ActiveRecord::Base
  has_one :address
  delegate :street_address, :to => :address</p>

<p>end</p>

<p>User.new.street_address # Calls address.street_address
```</p>

<p>Check out the <a href="http://apidock.com/rails/Module/delegate">delegate documentation</a> for more information on how this functionality works. But <a href="http://stackoverflow.com/questions/9914400/delegate-all-method-calls-on-a-model-to-an-association">a question I came across on Stack Overflow</a> today asked: what do you when you want to delegate all methods? I do <a href="https://github.com/Veraticus/Dynamoid/blob/master/lib/dynamoid/adapter.rb#L122">something similar in Dynamoid</a> and wanted to talk about how to make this pattern sensible and performant.</p>

<!-- more -->


<p>Essentially you have two options when delegation fails: the easy but less performant way, and the hard but more performant way. It's always nice to have choices, right?</p>

<h2>The Easy Way</h2>

<p>The easy way uses <code>method_missing</code>. Method_missing, of course, is part of Ruby's extensive metaprogramming suite; it is called when a method that doesn't exist is invoked on an object. So, if you have an object (say our user from above) and you want to delegate all methods that it doesn't have itself to its address, you would simply do:</p>

<p>```ruby
class User &lt; ActiveRecord::Base
  has_one :address</p>

<p>  def method_missing(method, *args)</p>

<pre><code>return address.send(method, *args) if address &amp;&amp; address.respond_to?(method)
super
</code></pre>

<p>  end
end
```</p>

<p>This works and will correctly send every method that can be called on an address to that address. Unfortunately, method_missing is slower than defining a method directly on the object, so every time you're forced to use method_missing you're added fractions of milliseconds to your application. This speed difference is usually imperceptible, but you never know: if you use this method_missing enough it could make a difference.</p>

<h2>The Hard Way</h2>

<p>So we have the hard way. This method is "harder" only in that you need to know the methods you want to delegate beforehand -- in which case, why aren't you using <code>delegate</code>? There is still a use for this method though: if you want to do something like benchmarking or argument recording before you delegate, you can do that easily here.</p>

<p>```ruby
class User &lt; ActiveRecord::Base
  has_one :address</p>

<p>  [:all, :my, :methods, :here].each do |m|</p>

<pre><code>define_method(m) do |*args|
  address.send(m, *args)
end 
</code></pre>

<p>  end</p>

<p>  end
```</p>

<p>In Dynamoid, we perform benchmarking in this method before sending the response along, allowing you to see how long the actual request took in DynamoDB.</p>

<p>Ultimately, you should use <code>delegate</code> if possible... but if it isn't, then either of these two options should get you started to avoid programming unpleasantness. Don't address your objects through other objects -- your code will look better and be more maintainable if you take some time to isolate methods!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch and Percolation in Rails]]></title>
    <link href="http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails/"/>
    <updated>2012-03-25T11:39:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails</id>
    <content type="html"><![CDATA[<p>Hipstamatic uses the pretty awesome Family Album feature for people to like and react to each others' photos. You can create either a magic album -- an album that matches to a combination of criteria including accounts, geolocation, tags and descriptions -- or a curated album, selecting photos directly that you want to include. The latter is a pretty straight-forward association and isn't very interesting to talk about, but I wanted to discuss briefly the methods we used to implement magic albums and what we finally settled on. It involved a lot of setting up elasticsearch and percolation, and ultimately I think it's a very durable, excellent solution for anyone wanting to index a lot of data and retrieve it extremely quickly.</p>

<!-- more -->


<p>Initially, magic albums were a set of complicated MySQL queries. I think anyone who's had experience with indexes in an enormous MySQL database knows where this one is going... its performance was terrible, and as more people created more albums our RDS instance started really chugging. The worst part was we were spending enormous amounts of time, energy, and money invested in a small part of our application, and it was having a cascade effect through the database ruining the rest of the user experience.</p>

<p>As a stopgap measure, we switched to using Redis lists to hold the association but kept the actual index in MySQL. Recently though we migrated away from MySQL completely to an index storage called <a href="http://www.elasticsearch.org/">elasticsearch</a>. Elasticsearch is awesome because it's built on Lucene, is incredibly easy to get going, and is very very powerful. I passed over search solutions like <a href="http://sphinxsearch.com/">Sphinx</a> and <a href="http://www.searchify.com/">Searchify</a> mostly because we aren't doing any text searching: all of the queries albums perform on photos are controlled by direct, matched fields. We just needed a great, simple engine for indexing them constantly and pulling results out quickly -- an engine that wouldn't bring the rest of our stack down if there was an indexing failure or if we were bombarded with many simultaneous queries.</p>

<p>Elasticsearch has given us all that and more. Using the amazing <a href="https://github.com/karmi/tire">tire</a> gem, it was simple to get our photo model set up correctly:</p>

<p>```ruby
class Photo &lt; ActiveRecord::Base
  include Tire::Model::Search</p>

<p>  mapping do</p>

<pre><code>indexes :id
indexes :lat_lng, :type =&gt; :geo_point
indexes :account_id
indexes :created_at, :type =&gt; :date
indexes :tags
</code></pre>

<p>  end
end
```</p>

<p>(The code here is changed slightly from its production form to redact business logic and simplify it.) Of course, the real magic takes place in the albums model. Albums are essentially saved queries, if you think about it: they should search for photos every time they're called. So we have a method to generate the query we're looking for:</p>

<p>```ruby
class Album &lt; ActiveRecord::Base</p>

<p>  def elasticsearch_query</p>

<pre><code>query = []
query &lt;&lt; {:terms =&gt; {:account_ids =&gt; query[:accounts]}} unless query[:accounts].blank?
query &lt;&lt; {:terms =&gt; {:tags =&gt; query[:tags]}} unless query[:tags].blank?
query &lt;&lt; {:range =&gt; {:created_at =&gt; {:from =&gt; query[:starts_at], :to =&gt; query[:ends_at]}}} unless query[:starts_at].blank? &amp;&amp; query[:ends_at].blank?
query &lt;&lt; {:geo_distance =&gt; {:lat_lng =&gt; [query[:lat].to_f, query[:lng].to_f.join(','), :distance =&gt; "#{query[:range]}km"}} unless query[:lat].blank? || query[:lng].blank?
query
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>These are all, in elasticsearch parlance, filters rather than queries: queries must look into data fields and perform operations in them, whereas filters just filter on a fields' value directly... exactly what I was looking for. <code>terms</code> instructs the filter parser that at least one of the select elements must match. <code>range</code>, as you can see, allows us to pull only photos within a certain date. <code>geo_distance</code> is particularly cool and lets us filter all photos by their geographic location.</p>

<p>Using this couldn't be simpler:</p>

<p>```ruby
class Album &lt; ActiveRecord::Base</p>

<p>  def elasticsearch_photos</p>

<pre><code>finder = Photo.search do
  query { all }
  filter(:and, elasticsearch_query) unless elasticsearch_query.empty?
  sort {by :created_at, "desc" }
end

finder.results
</code></pre>

<p>  end
```</p>

<p>Tada! Easy and simple searching inside your models. The performance gain for us was massive; elasticsearch has a ridiculously small memory footprint, but consistently returns responses to us in 50-60 milliseconds. Now that's performance!</p>

<p>Many of you might be wondering, though, how we get the reverse of this association. Albums have many (searched) photos; how does a photo know what album it's in? This was a stumbling block for the other search solutions I investigated, and I was concerned I would have to bust out the old, gimpy MySQL.</p>

<p>But elasticsearch to the rescue! It employs a very neat feature called <a href="http://www.elasticsearch.org/blog/2011/02/08/percolator.html">the percolator</a>. Percolation allows us to save searches as an index themselves, and then determine what objects match any of the saved searches. So, we save the search an album would conduct along with the album's ID into the photo percolator; then we can determine what queries a photo matches when we save it. It's really quite ingenuous and was, of course, ridiculously easy to set up:</p>

<p>```
class Album &lt; ActiveRecord::Base
  after_save :register_query</p>

<p>  def register_query</p>

<pre><code>Photo.index.register_percolator_query(self.id) do |q|
  q.filtered do
    query {all}
    filter(:and, elasticsearch_query) unless elasticsearch_query.empty?
  end
end
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>This uses the same <code>elasticsearch_query</code> method as above (of course, since we want to save the same query into the database). And on the photo model, to use it, we just do:</p>

<p>```
class Photo &lt; ActiveRecord::Base</p>

<p>  def percolated_albums</p>

<pre><code>Album.find(Photo.index.percolate(self))
</code></pre>

<p>  end
end
```</p>

<p>This was a rather whirlwind tour, but I was really impressed at how easy it was to get elasticsearch set up properly. It really has added quite a lot to our stack and I look forward to using it on other domain problems (maybe even including full text search)! It was pretty easy to get it tested as well, but I think I'll save details on how I did that for another post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sweeping Caches from Resque (or Anywhere Really)]]></title>
    <link href="http://joshsymonds.com/blog/2012/03/19/sweeping-caches-from-resque-or-anywhere-really/"/>
    <updated>2012-03-19T10:29:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/03/19/sweeping-caches-from-resque-or-anywhere-really</id>
    <content type="html"><![CDATA[<p>Phil Karlton, someone I can only presume is a pretty smart programmer, said <a href="http://martinfowler.com/bliki/TwoHardThings.html">"there are only two hard things in Computer Science: cache invalidation and naming things."</a> He's totally right; cache invalidation is one of the biggest headaches when designing highly usable, highly available websites and is something that I'm sure every Rails programmer worth their salt has struggled with. (Naming things is also a pain but not the focus of this post.)</p>

<!-- more -->


<p>And unfortunately the reason for the struggle is that Rails' caching tools don't go nearly as far as they should. This is really through no fault of their own; honestly, Rails' caching methods are amazingly robust, and if you don't know what they are, you should read <a href="http://guides.rubyonrails.org/caching_with_rails.html">the guide</a> on them. But good tools can only take you so far. Ultimately, caching is as application-specific as you can get, and when you get to finely-grained control you have to take the reins yourself.</p>

<p>One of the problems I ran into recently was invalidating caches during an association join. I have two models, album and photo, and when one is added to the other I wanted to expire all the caches dealing with both. I already have <a href="http://api.rubyonrails.org/classes/ActionController/Caching/Sweeping.html">cache sweepers</a> in my application, but callbacks aren't triggered on association. And putting something in an after_add on the association itself didn't seem like the right answer; why should I put cache expiration stuff in my model when I already have sweepers dedicated to that logic?</p>

<p>I'm not sure I'm in love with the solution I came up with, but it certainly seems to work. All of the association logic happens in Resque jobs, so I added the cache invalidation directly to this jobs by invoking the sweeper manually:</p>

<p><code>ruby
PhotoSweeper.send(:new).expire_cache_for(photo)
</code></p>

<p>The <code>send</code> business is necessary because new is a private method for sweepers. Nevertheless this really seems to get the job done; the caches are swept appropriately, and my cache invalidation logic remains safely in the sweepers, where I can add or edit it as much as I want. I suppose if I really wanted to I could put this in an after_add on the model as well. I've resisted that so far but maybe it's the logical place for this kind of expiration logic to happen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fnordmetric: Native Rails Metrics]]></title>
    <link href="http://joshsymonds.com/blog/2012/03/13/fnordmetric-native-rails-metrics/"/>
    <updated>2012-03-13T22:05:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/03/13/fnordmetric-native-rails-metrics</id>
    <content type="html"><![CDATA[<p>Over the weekend I spent some time getting <a href="https://github.com/paulasmuth/fnordmetric">Fnordmetric</a> set up in an application. On the surface it looks really cool and quite nifty, but I ran into some trouble getting it configured how I wanted it and figured I'd make a post about it. I think I might have been struggling against the conventions in it a little too much, but it was still an instructive battle.</p>

<!-- more -->


<h2>Engineize It</h2>

<p>The Gem itself assumes you'll be running it on its own port, presumably redirecting nginx traffic there. But this is 2012 and Rails engines are all the rage -- so why bother with a separate app? Well, I'll get to the reason why later, but mounting it as an engine is pretty simple.</p>

<p>Set up an initializer or something that defines all the Fnord metrics you want, something like <code>config/initializers/fnord.rb</code>
```ruby
require "fnordmetric"</p>

<p>FnordMetric.namespace :analytics do
  gauge :events_total,</p>

<pre><code>:tick =&gt; 1.day.to_i, 
:progressive =&gt; true,
:title =&gt; "Events (total)"
</code></pre>

<p>  event(:"*") do</p>

<pre><code>incr :events_tota
</code></pre>

<p>  end
end</p>

<p>FnordMetric.server_configuration = {
  :redis_url => "redis://localhost:6379",
  :redis_prefix => "fnordmetric",
  :inbound_stream => ["0.0.0.0", "1339"],
  :start_worker => true,
  :print_stats => 3,
  :event_queue_ttl => 120,
  :event_data_ttl => 3600<em>24</em>30,
  :session_data_ttl => 3600<em>24</em>30
}
```</p>

<p>That stuff is copy-pasted from the Github README, so I won't go into explaining it. Note that we do not include FnordMetric.standalone at the bottom, however; we'll be mounting the server ourselves in routes.rb like so:</p>

<p><code>ruby
  mount FnordMetric.embedded, :at =&gt; "/fnord"
</code></p>

<p>Then you can go to localhost:3000/fnord, and tada! Fnord metrics!</p>

<h2>Set Up a Worker</h2>

<p>The difficult, of course, is that each instance of your app will now also spin up its own instance of a FnordWorker, which might not be what you want. I got around this by altering my config/initializers/fnord.rb:</p>

<p><code>ruby
FnordMetric.server_configuration = {
  :redis_url =&gt; "redis://localhost:6379",
  :redis_prefix =&gt; "fnordmetric",
  :inbound_stream =&gt; ["0.0.0.0", "1339"],
  :start_worker =&gt; (Rails.env.development? || ENV['FNORD_WORKER'] ? true : false),
  :print_stats =&gt; 3,
  :event_queue_ttl =&gt; 120,
  :event_data_ttl =&gt; 3600*24*30,
  :session_data_ttl =&gt; 3600*24*30
}
</code></p>

<p>I know some people hate the ternary operator, but I kind of like it. Anyway, this causes the worker to start only if there's an environment variable set to start it or the Rails environment is development. I set up one instance that receives this variable when it starts, and now I only have one worker. Simplicity itself!</p>

<p>Ultimately, I like Fnordmetric, but I'm not using it in my production applications. I feel like there's a level of abstraction to go before it's really usable in big production apps. It's much better at tracking arbitrary metrics than NewRelic -- honestly, trying to shoehorn stats into their system feels silly at times -- but setting up the tracking stuff is a pain, involving a lot of unnecessary repetition. I think that a Fnordmetric2.0 would be awesome, though, so I hope the project sees more love and work. And who knows, if I have some time I'll try contributing to it myself. That's the joy of open source: if you have a good idea, you make it happen.</p>
]]></content>
  </entry>
  
</feed>
