<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: servers | Josh Symonds]]></title>
  <link href="http://joshsymonds.com/blog/categories/servers/atom.xml" rel="self"/>
  <link href="http://joshsymonds.com/"/>
  <updated>2012-04-09T22:42:10-05:00</updated>
  <id>http://joshsymonds.com/</id>
  <author>
    <name><![CDATA[Josh Symonds]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Achieving 100% Uptime]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime/"/>
    <updated>2012-04-09T17:52:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime</id>
    <content type="html"><![CDATA[<p><img src="http://f.cl.ly/items/0q2M3B2o1f1D1D451B1S/uptime.jpg" alt="Uptime - 100%" /></p>

<p>Keeping a highly available web application online is no joke. Everything above 99% is extremely impressive; that means that you battled the forces of <a href="http://en.wikipedia.org/wiki/Software_erosion">erosion</a> and probably even deployed some pretty neat features without even a hiccup from your users' perspective. I always feel great when I get our weekly <a href="http://newrelic.com/">New Relic</a> status report email -- it's a good indication of how well I did my job in the previous week. And for a couple weeks now I'm happy to report I've been very proud indeed, with 100% uptime on the Hipstamatic web application.</p>

<p>How do you achieve numbers like these? Unfortunately getting to 100% isn't an easy road, and I want to state up front that I also don't think it's a realistic goal. Issues you can't control can ruin your uptime number, and you shouldn't feel broken up about that. It happens to everybody. But it's always good setting goals that are difficult to achieve, and this one is no different.</p>

<p>So what's the secret to 100% uptime?</p>

<!-- more -->


<h2>Watch It Constantly</h2>

<p>Some people check their fantasy baseball league or their portfolio every morning. At the slightest hint of trouble, they'll be waist-deep in trading players or stocks to get everything right back on track. You should be that way with your servers and the software that runs on them. This usually means monitoring software, and a lot of it.</p>

<p>At Hipstamatic, we make extensive use of New Relic to give us a broad overview of our application. It helps us proactively fix nascent problems, analyzing slow queries and sluggish pages. But you need something closer to the metal, and for that we use <a href="http://mmonit.com/monit/">monit</a>. Monit is an amazing tool to control your applications' behavior and warn you when that behavior becomes dangerous. Here's a sample of our unicorn monit file:</p>

<p><code>bash
  if totalmem &gt; 70% for 4 cycles then alert
  if totalmem &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
  if cpu &gt; 70% for 4 cycles then alert
  if cpu &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
</code></p>

<p>This states that I get an alert when unicorn's total memory or CPU usage exceeds 70%, and that unicorn receives a USR2 signal when total memory or CPU exceed 90%.</p>

<p>Finally, we employ <a href="http://munin-monitoring.org/">munin</a> to compile statistics that we care about, including nginx connections and unicorn requests served.</p>

<p>Yes, this is a lot of monitoring. But I feel like even this isn't enough. You can't watch your stack too carefully, and you can't have too many tools in place to help you analyze what's going on. Consider this trifecta of tools only a start, but at least it's a good one.</p>

<h2>Seamless Deploys</h2>

<p>On an average week I deploy seven to ten times. Of course, this entire process is invisible to our users; the magic that makes this happen is <a href="http://unicorn.bogomips.org/">unicorn</a>. There have been many posts on the wonders of unicorn and how to configure it correctly. I will simply post the part of our <code>unicorn.rb</code> that allows us to do seamless restarting, which you can find in a number of gists essentially unmodified.</p>

<p>```ruby
before_fork do |server, worker|
  old_pid = "#{server.config[:pid]}.oldbin"
  if File.exists?(old_pid) &amp;&amp; server.pid != old_pid</p>

<pre><code>begin
  Process.kill("QUIT", File.read(old_pid).to_i)
rescue Errno::ENOENT, Errno::ESRCH
end
</code></pre>

<p>  end
end
```</p>

<p>The command we use to restart unicorn is:</p>

<p><code>bash
  if [ ! -f '/pids/unicorn.pid' ]; then cd current_path &amp;&amp; bundle exec unicorn_rails -c ./config/unicorn.rb -E production -D; else kill -USR2 `cat /pids/unicorn.pid`; fi
</code></p>

<p>USR2 is the signal that tells unicorn to start reloading itself: the before_fork causes the new server to kill the old server only when it's ready to start processing connections.</p>

<h2>Migrations Without Downtime</h2>

<p>The last key component to 100% uptime is migrating your database without bringing your site down. Of course, this only applies if you're changing how existing code interacts with the database -- for new tables, simply migrate before deploying and you're done. If only it could be that easy all the time...</p>

<p>Frequently we are required to change existing tables and colums or add new ones. For those of us still using relational databases, migrations almost always mean locked tables, and locked tables mean site downtime. To fix this problem, my tool of choice has been <a href="https://github.com/soundcloud/large-hadron-migrator">Large Hadron Migrator</a>. Large Hadron Migrator requires very little from your tables (just an autoincrementing ID) and allows you to alter tables and even add new columns without bringing your site down.</p>

<p>```ruby
class AddOrdersCountToUsers &lt; ActiveRecord::Migration</p>

<p>  def self.up</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} ADD COLUMN orders_count INT(11) default 0")
end
</code></pre>

<p>  end</p>

<p>  def self.down</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} DROP COLUMN orders_count")
end
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>Yes, unfortunately, this includes raw SQL. There is a <a href="http://rubydoc.info/github/soundcloud/large-hadron-migrator/master/Lhm/Migrator#add_column-instance_method">small DSL</a> that exposes a few common methods, but for anything really deep you're gonna need to get your hands dirty. Using this method you'll be able to become the envy of your friends and peers, for you'll be able to execute zero downtime migrations.</p>

<p>And those three points are the main ways I've reduced our downtime. It's a difficult road to 100%, but it's worth it because you can stare at pretty graphs like this:</p>

<p><img src="http://f.cl.ly/items/470B350J0U0q1u3r0T0s/availability-1.jpg" alt="Better than Facebook" /></p>

<p>And imagine that your website and a 100% bar are sitting right at the very tippy top.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How I Scaled Hipstamatic]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/06/how-i-scaled-hipstamatic/"/>
    <updated>2012-04-06T10:58:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/06/how-i-scaled-hipstamatic</id>
    <content type="html"><![CDATA[<p>The <a href="http://joshsymonds.com/blog/2012/04/03/how-to-refactor-a-large-and-old-project/">Proper Refactoring</a> proceeds apace, but I think in my last post I gave the impression that the Hipstamatic Rails project is inefficient or, even worse, slow. Nothing could be further from the truth; over the course of two years I've been continuously improving the project to be more responsive and much, much faster. How much faster? Well, unfortunately, I don't have metrics from the first months I worked at Synthetic. But we were using XML and then plist to generate our responses to the iPhone app, and that process was achingly slow: I would estimate 200ms on average.</p>

<p>Now, take a look at our average response time over the last month.</p>

<p><img src="http://f.cl.ly/items/1H0v0C420X0a1O3Y2p3Y/Hipstaweb%20-%20New%20Relic-1.jpg" alt="Average response time - 115ms" /></p>

<p>Considering the web of external services Hipstamatic depends on for much of its operation, I'm proud of our 115ms average response time. Proud but not satisfied -- hence the need for the Proper Refactoring, and I am optimistic that it will lead to a net performance gain for us and our users. There's no reason we can't achieve 50-70ms response times with better caching and slimmer applications.</p>

<p>Over the same time period that our response time has dropped, our user base has grown exponentially, and so too our traffic. At the beginning of my tenure at Synthetic our site was receiving close to 100,000 hits a day, and nearly all of that web traffic: now <a href="http://community.hipstamatic.com">community.hipstamatic.com</a> sees about a million requests a day, most of that API traffic generated from our iPhone applications. That's an enormous amount of growth, and much of that over the course of just one or two explosive months.</p>

<p>Synthetic is <a href="http://heysynthetic.com/about_us/">a team</a> of extremely talented individuals. But as our main Rails programmer and only server administrator, I wanted to discuss the lessons I personally learned in making Hipstamatic's web site and web services fast. (Or, at the very least, a lot faster.)</p>

<!-- more -->


<h2>Cache <em>Everything</em></h2>

<p>This is easily the most important, most crucial rule to making your applications fast. You'd be surprised what you can cache, and how much time caching will save. Memcached access times are ridiculously fast, faster even than the fastest database query. Stick everything in your cache. Everything. <em>Everything</em>.</p>

<p><img src="http://f.cl.ly/items/3f0J2M0H3o3h0w1X1i3m/cache-all-the-things.jpg" alt="Cache all the things" /></p>

<p>This is such an important rule I even gave you an annoying memegenerator image of it. Yes, people, it's memegenerator important.</p>

<p>You really can't go overboard enough when it comes to caching. Make resque jobs whose only purpose is to warm your caches. Use <a href="http://api.rubyonrails.org/classes/ActionController/Caching/Sweeping.html">cache sweepers</a> to sensibly and programmatically expire caches. Almost 90% of our application traffic returns the same (or very similar) JSON and HTML. By caching those responses, we save massive amounts of time, and more importantly, boatloads of money (due to lessened server load).</p>

<h2>Choose Your Tools Wisely</h2>

<p>Choose software that is frequently updated and widely used in the community. Choose software that is robust: by that I mean resistant to failure, and that has survived years of use in live, highly available environments. And finally and most importantly, choose software that is fast. Very, very fast.</p>

<p>Two years ago, we switched from a single, shared instance on A Small Orange to Amazon's Elastic Cloud, allowing us to scale each of our components as necessary and independently from each other. Speed gains were noticeable immediately, but even better was the fact that there was a whole bunch of excellent software easily available to help us manage and scale our cloud presence. (See my earlier post <a href="http://joshsymonds.com/blog/2012/02/23/why-i-like-rubber/">on Rubber</a>).</p>

<p>One year ago, we migrated from Apache and Passenger to nginx and Unicorn. I don't want to get into software evangelism or drawn out discussions about which server software is superior; for our stack, for our requirements, nginx and Unicorn are much faster and more memory efficient than Apache and Passenger ever were. And more responsive -- the ability of Unicorn to do live deploys is just amazing and has totally revolutionized our development and deployment process.</p>

<p>Take the time and do the research. There's a best tool for you waiting out there -- finding it will make your life a thousand times easier.</p>

<h2>Less is More</h2>

<p>For a long time, incoming requests to the app were load balanced through <a href="http://haproxy.1wt.eu/">HAProxy</a> before reaching a Passenger instance. HAProxy is an amazing piece of software; it's extremely fast and gives you an awesome drill-down into incoming requests and your server status.</p>

<p>It also added 10 milliseconds to our response times on average.</p>

<p>If a piece of your stack isn't mission critical (and HAProxy, for us, was just a nice piece of software and not mission critical) then you should remove it. Amazing graphics and interesting metrics are less important than your response time. Examine your stack carefully, with a very critical eye, and whatever isn't absolutely necessary I would strongly recommend cutting out entirely.</p>

<p>What I found helpful to do was draw a quick flow chart of how a request is actually serviced. Nothing that you intend to present to your boss; just a small approximation of your stack. Each step on that chain adds time to that request returning a response. If it adds time to the request turnaround, it needs to be adding something important to that response. Otherwise, it needs to go.</p>

<h2>Achieve Balance</h2>

<p>When we were extensively using <a href="http://redis.io/">redis</a> as a semi-persistent datastore, I constantly experienced bottlenecks for redis connections. But you can encounter this problem anywhere in your server setup: I also had to deal with MySQL bottlenecks and, in one extremely memorable instance, Unicorn queue bottlenecks. These are all issues with load balancing inside the stack.</p>

<p>There's never a part of your stack that is immune to load balancing problems. Once you correctly scale one part, another component that performed adequately will suddenly start chugging under unexpected load or new use conditions. And, unfortunately, pre-optimization can backfire; sometimes you'll target the wrong part of your stack for optimizations, and other times you'll scale something that won't experience a bottleneck at all.</p>

<p>I recommend against trying to pre-scale unless you're sure that a new feature will distribute existing load in new, exciting ways. Achieving balance is an ongoing tightrope act -- you can guess to a limited extent where you'll tip after the next step, but you can never be sure until you actually take it. That's why being sensitive to your application after changes is so important. Use <a href="http://newrelic.com/">New Relic</a> to monitor your setup very carefully, especially after deploys, and have plans in place to scale every component of your application if necessary.</p>

<p>Formal plans generally aren't required, but know what steps you'd take if something started to fail. Even ten seconds of idle thought can save you agonizing minutes of unavailability.</p>

<h2>Use 75% of Every Server</h2>

<p>This rule applies doubly to servers on EC2. Instances that reach 100% memory or CPU utilization are instances that are very difficult to fix (and are much more prone to crashing in a shared environment). You can't SSH into them because they take forever to respond; you can't reboot them because they don't respond to Amazon's control plane. They are about to become horrible zombies in your setup, taking up space but refusing to die, and you'll have to route around them to keep your uptime intact.</p>

<p>Try to ensure your servers never reach this stage. I try to keep my computers at either 75% CPU utilization or 75% memory utilization: achieving both simultaneously is a very difficult balancing act but if you can get there then I applaud you. (As a side note, this is why Heroku is so appealing to me -- not needing to worry about maximizing your server resources sounds pretty awesome.)</p>

<p>If you're using less than 75%, then you can likely combine services together and remove servers. And if you're using more... well, I have <a href="http://www.pagerduty.com/">PagerDuty</a> configured to call me if at any time a server reaches 85% resource usage, and those are calls I take very seriously.</p>

<p>I'm sure I'll think of other lessons I learned while scaling Hipstamatic. Many of these ideas are shared ideas -- for example, the amazing <a href="http://samsoff.es/">Sam Soffes</a> initially encouraged us to move from Apache/Passenger to Nginx/Unicorn. However, the implementation and maintenance was mine and mine alone, and boy did I learn a lot while scaling Hipstamatic.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Unicorn in a Production Environment]]></title>
    <link href="http://joshsymonds.com/blog/2012/02/27/setting-up-unicorn-in-a-production-environment/"/>
    <updated>2012-02-27T10:20:00-06:00</updated>
    <id>http://joshsymonds.com/blog/2012/02/27/setting-up-unicorn-in-a-production-environment</id>
    <content type="html"><![CDATA[<p>Configuring unicorn for your Rails servers is as much an art as it is a science.</p>

<p>That said, there are some things that make the configuration and setup a lot easier that I wish I had known before I had taken the unicorn plunge.</p>

<!-- more -->


<h2>worker_processes</h2>

<p>I searched high and low for a guide on how many workers each of my unicorns should employ and came up empty-handed. Unfortunately, this is highly dependent on your application. If you do complicated ImageMagick or PDF transformations on your server threads (which you shouldn't with unicorn, but hey, who knows) then your threads will use a lot of memory, especially on those operations. For reference, our Rails app takes up about 25 megabytes of memory per worker. However, we have offloaded all of our long-running and memory-intensive tasks into Resque.</p>

<p>We use EC2's m1.xlarge instance class and have 30 workers running per server. This number is intentionally set low; in my experience, the closer you come to maximum resource usage in an EC2 instance, the more likely it is to crash, or even worse become unresponsive.</p>

<h2>working_directory</h2>

<p>This should be fairly straightforward but has an important caveat: make sure to make this the static path of your current deploy, so the actual target of your symlink. Ours is <code>'/our/app/directory/current'</code>. Putting fanciness in here is very likely to get you shot in the foot with unicorn not reloading your app correctly, so I can't stress enough, just make this a simple string.</p>

<h2>listen</h2>

<p>Our backlog is set to 64. If a unicorn has 64 queued connections likely it's dead and we need failover to happen immediately; nginx takes care of that when unicorn refuses to service a request.</p>

<h2>preload_app</h2>

<p>true.</p>

<h2>Gemfile</h2>

<p>We had an issue where unicorn wouldn't pick up our Gemfile correctly. Turns out that it doesn't understand symlinked directories for reading gemfiles, so we had to employ this dazzling bit of code to get new gems into our bundle when the unicorns restarted:</p>

<p><code>ruby
before_exec do |server|
  ENV['BUNDLE_GEMFILE'] = '/mnt/Hipstaweb-&lt;%= RUBBER_ENV %&gt;/current/Gemfile'
end
</code></p>

<h2>before_fork and after_fork magic</h2>

<p>If you're seriously considering unicorn, I'm sure you've seen the following gisted:</p>

<p>```ruby
before_fork do |server, worker|
  defined?(ActiveRecord::Base) and</p>

<pre><code>ActiveRecord::Base.connection.disconnect!
</code></pre>

<p>  old_pid = "#{server.config[:pid]}.oldbin"
  if File.exists?(old_pid) &amp;&amp; server.pid != old_pid</p>

<pre><code>begin
  Process.kill("QUIT", File.read(old_pid).to_i)
rescue Errno::ENOENT, Errno::ESRCH
end
</code></pre>

<p>  end
end
```ruby</p>

<p>But just to reinforce how awesome it is: this configuration is awesome. It allows your new unicorn to gracefully kill the old master while a new one seamlessly reloads. This is the key to one of unicorn's biggest selling points -- zero downtime deploys.</p>

<p><code>ruby
after_fork do |server, worker|
  ActiveRecord::Base.establish_connection
  ActiveRecord::Base.verify_active_connections!
end
</code></p>

<p>If you include the <code>connection.disconnect!</code> line in your before_fork, make sure you reestablish the connection in your after_fork. This prevents stale database connections and ensures each worker is always correctly connected to the database.</p>

<p>(As a postscript to this post: your unicorn servers are required to have <a href="http://en.wikipedia.org/wiki/List_of_My_Little_Pony:_Friendship_Is_Magic_characters">My Little Pony</a> server names.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why I Like Rubber]]></title>
    <link href="http://joshsymonds.com/blog/2012/02/23/why-i-like-rubber/"/>
    <updated>2012-02-23T13:30:00-06:00</updated>
    <id>http://joshsymonds.com/blog/2012/02/23/why-i-like-rubber</id>
    <content type="html"><![CDATA[<p>When I was analyzing tools to deploy applications into Amazon's elastic cloud, I settled on <a href="https://github.com/wr0ngway/rubber">rubber</a> as our EC2 provisioner of choice.</p>

<p>rubber has a lot going for it. It provides simple, close-to-the-metal server creation and bootstrapping; it's super easy to start a server with nothing more than:</p>

<p><code>bash
cap rubber:create ALIAS=fluttershy ROLES=unicorn
</code></p>

<p>And bootstrap it with all the software needed for that role with:</p>

<p><code>bash
cap rubber:bootstrap FILTER=fluttershy
</code></p>

<p>Of course, looking at command line magic doesn't tell you anything about how hard or easy it is to initially configure, but rubber is also pretty easy to figure out and get running. When you initially rubber-ize your project, you get an entire directory in config/ called rubber that contains all the configuration files, separated by role, that you can browse to figure out exactly what rubber is doing.</p>

<p>It's easy to extend, as well. Adding in hubot scripts to automatically start and stop our little Hipstabot was really painless and will make a good future post, I'm sure. The good part -- for me at least -- is how close it is to the servers and how well it ties in with Capistrano. There's no magic going on and there's almost nothing to learn: as long as you have an EC2 account, you can set up a server quickly and painlessly with rubber... but if you know what you're doing, it provides an immense amount of flexibility and power to deploy whatever you want in a simple, repeatable way.</p>

<p>rubber isn't just happiness and unicorns, though; it has some definite downsides. It seems to have a lot less mindshare than tools like Chef or Puppet, and because it's deployed on a per-application basis, it can't easily manage multiple applications deployed onto one machine. But despite these problems I like it a lot, and it perfectly suits our present use case.</p>
]]></content>
  </entry>
  
</feed>
