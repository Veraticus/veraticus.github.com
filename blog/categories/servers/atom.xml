<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: servers | Hi, I'm Josh Symonds]]></title>
  <link href="http://joshsymonds.com/blog/categories/servers/atom.xml" rel="self"/>
  <link href="http://joshsymonds.com/"/>
  <updated>2013-06-11T17:24:35-05:00</updated>
  <id>http://joshsymonds.com/</id>
  <author>
    <name><![CDATA[Josh Symonds]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pricing Popular Hosting Options (With Devops Time)]]></title>
    <link href="http://joshsymonds.com/blog/2013/04/17/pricing-popular-hosting-options-with-devops-time/"/>
    <updated>2013-04-17T18:16:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2013/04/17/pricing-popular-hosting-options-with-devops-time</id>
    <content type="html"><![CDATA[<p>Recently I compared the major Rails hosting providers -- but as opposed to most price breakdowns I've read on the Internet, I opted to include provisional hourly devops time to set up and perform maintenance on the servers. For the purposes of this comparison, I only selected four providers: AWS, RackSpace, BlueBox and Heroku, and I'm assuming you use all their services (rather than combining two, say Heroku Postgres with AWS EC2 instances). I found the resulting price breakdown instructive, though interpreting them (and disagreeing with the provided hours) are left as an exercise for the reader.</p>

<!-- more -->


<h2>Comparisons</h2>

<p>Any of these configurations should be adequate to support roughly a million requests a month (assuming throughput of 5 requests a second), provided most of the requests served aren't that complicated. We'll go for a medium database instance and aggressively cache as much as possible, thus we'll also need to provide memcached room somewhere.</p>

<p>The big differentiator in my comparison (as opposed to others') is certainly a devops contractor at $150 an hour. I'll include the hours as I would estimate them personally, but for other people it might take longer or shorter -- and the price could go up if there's a ton of other software to go in the server. (For example, this theoretical application would probably eventually want redis and some sort of asynchronous worker system.)</p>

<p>So let's get down to the details!</p>

<h3>Amazon Web Services</h3>

<table class='numbers'>
  <tr>
    <th style='width: 72%;'>Service</th>
    <th>Setup</th>
    <th>Monthly</th>
  </tr>
  <tr>
    <td>
      <h4>1 medium EC2 instance (1 year contract, medium utilization)</h4>
      6 unicorn workers<br/>
      1 nginx reverse proxy<br/>
      memcached
    </td>
    <td>$277.00</td>
    <td>$30.74</td>
  </tr>
  <tr>
    <td>
      <h4>1 medium RDS instance (1 year contract, medium utilization)</h4>
    </td>
    <td>$500.00</td>
    <td>$40.26</td>
  </tr>
  <tr>
    <td>
      <h4>Devops Time</h4>
      10 hours setup<br />
      5 hours maintenance
    </td>
    <td>$1500.00</td>
    <td>$750.00</td>
  </tr>
  <tr class='highlighted'>
    <th>Total</th>
    <th>$2277.00</th>
    <th>$821.00</th>
  </tr>
  <tr class='highlighted'>
    <th>First Year</th>
    <th colspan='2'>$12129.00</th>
  </tr>
</table>


<p>No surprises here: if you're using AWS, the hardware is ridiculously cheap. Most of your cost is going to be engineering time to get the instance up and running and then perform maintenance and add additional features to it. That said, I've had an EC2 instance going for about 8 months now with no maintenance at all on my part (laziness!), so if you don't need any additional server setup you can probably omit the maintenance time, for a monthly cost of $71.00 and a yearly cost of $3129.00.</p>

<h3>RackSpace</h3>

<table class='numbers'>
  <tr>
    <th style='width: 72%;'>Service</th>
    <th>Setup</th>
    <th>Monthly</th>
  </tr>
  <tr>
    <td>
      <h4>1 4GB managed cloud instance</h4>
      6 unicorn workers<br/>
      1 nginx reverse proxy<br/>
      memcached
    </td>
    <td>$0.00</td>
    <td>$262.80</td>
  </tr>
  <tr>
    <td>
      <h4>1 4GB cloud database instance</h4>
    </td>
    <td>$0.00</td>
    <td>$321.20</td>
  </tr>
  <tr>
    <td>
      <h4>Devops Time</h4>
      10 hours setup<br />
      2 hours maintenance
    </td>
    <td>$750.00</td>
    <td>$300.00</td>
  </tr>
  <tr class='highlighted'>
    <th>Total</th>
    <th>$750.00</th>
    <th>$884.00</th>
  </tr>
  <tr class='highlighted'>
    <th>First Year</th>
    <th colspan='2'>$11358.00</th>
  </tr>
</table>


<p>RackSpace's managed cloud offerings are more expensive than AWS, but the theory is you can omit server-related maintenance (since they'll keep services running and your servers themselves operational) and that's reflected in a lowered monthly devops cost. They don't do maintenance or improvements on your application proper, however, so I built a rather modest two hours a month in for simple tasks like upgrading Rails or performing minor server optimizations. You can once again probably ignore the monthly devops cost if you like, but that won't have nearly the impact on the final price that it did for AWS, with a new monthly of $584.00 and a final year total of $7758.00.</p>

<h3>BlueBox</h3>

<table class='numbers'>
  <tr>
    <th style='width: 72%;'>Service</th>
    <th>Setup</th>
    <th>Monthly</th>
  </tr>
  <tr>
    <td>
      <h4>1 4GB cloud instance</h4>
      6 unicorn workers<br/>
      1 nginx reverse proxy<br/>
      memcached
    </td>
    <td>$0.00</td>
    <td>$385.00</td>
  </tr>
  <tr>
    <td>
      <h4>1 4GB cloud database instance</h4>
    </td>
    <td>$0.00</td>
    <td>$385.00</td>
  </tr>
  <tr>
    <td>
      <h4>Devops Time</h4>
      0 hours setup<br />
      0 hours maintenance
    </td>
    <td>$0.00</td>
    <td>$0.00</td>
  </tr>
  <tr class='highlighted'>
    <th>Total</th>
    <th>$0.00</th>
    <th>$770.00</th>
  </tr>
  <tr class='highlighted'>
    <th>First Year</th>
    <th colspan='2'>$9240.00</th>
  </tr>
</table>


<p>BlueBox's claim to fame is that they perform server, application, and database setup, maintenance, and integration. Thus the need for a devops engineer is completely obviated (as reflected in the final totals). Obviously this price point is extremely attractive if you'd otherwise have to pay a server administrator and engineer, but if you have one on staff already then BlueBox's product is easily the most expensive. You're paying for their expertise much more than their hardware.</p>

<h3>Heroku</h3>

<table class='numbers'>
  <tr>
    <th style='width: 72%;'>Service</th>
    <th>Setup</th>
    <th>Monthly</th>
  </tr>
  <tr>
    <td>
      <h4>4 dynos</h4>
      12 unicorn workers<br/>
    </td>
    <td>$0.00</td>
    <td>$143.00</td>
  </tr>
  <tr>
    <td>
      <h4>memcached addon (500 MB)</h4>
    </td>
    <td>$0.00</td>
    <td>$40.00</td>
  </tr>
  <tr>
    <td><h4>Fugu database instance</h4></td>
    <td>$0.00</td>
    <td>$400.00</td>
  </tr>
  <tr>
    <td>
      <h4>Devops Time</h4>
      2 hours setup<br />
      0 hours maintenance
    </td>
    <td>$300.00</td>
    <td>$0.00</td>
  </tr>
    <tr class='highlighted'>
    <th>Total</th>
    <th>$300.00</th>
    <th>$583.00</th>
  </tr>
  <tr class='highlighted'>
    <th>First Year</th>
    <th colspan='2'>$7296.00</th>
  </tr>
</table>


<p>I'm always somewhat mystified by Heroku's pricing -- their database offerings are incredibly expensive, especially compared to their incredibly cheap dynos. Anyway, they provide the least expensive option for purely hosting an application, but this cheapness comes with a hidden price. Being unable to control your production environment can be a frightening proposition and exposes you to potential hidden vagaries of Heroku's internals (such as the latest flap about their routing mesh). And the fact that their addons are third-party products means that if they go down, you have no ability to expedite their repair. I would deploy a small or medium app to Heroku (which might be perfect for this theoretical application), but for a bigger one I would definitely be hesitant.</p>

<h2>Conclusions</h2>

<p>I don't think any of these prices are particularly surprising. For knowledgeable server engineers, AWS is indeed a tremendous bargain. For those with little or no infrastructure knowledge, Heroku or BlueBox would be a much better choice. And keep in mind these are the hours it would take me to set up these instances; the times might not be representative of another engineer. I think they're reasonable though, and that the comparison is an interesting one to draw, even if not a tremendous revelation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why I Chose Chef Over Rubber]]></title>
    <link href="http://joshsymonds.com/blog/2013/01/18/why-i-chose-chef-over-rubber/"/>
    <updated>2013-01-18T14:34:00-06:00</updated>
    <id>http://joshsymonds.com/blog/2013/01/18/why-i-chose-chef-over-rubber</id>
    <content type="html"><![CDATA[<p>One of my mandates at Everest has been to sanitize the server build and deploy process. Provisioning every server individually with the same bash script was not exactly the height of extensibility and maintainability, and unfortunately had resulted in an enormous cluster that was very opaque: there was nearly no visibility into what the servers were actually doing. When I evaluated options to create a better process I looked at my go-to configuration management tool, <a href="https://github.com/wr0ngway/rubber">rubber</a>, in addition to <a href="http://en.wikipedia.org/wiki/Chef_(software)">Chef</a> and <a href="http://en.wikipedia.org/wiki/Puppet_(software)">Puppet</a>. As a result of this evaluation -- and surprising even myself -- I ended up choosing Chef as our solution. Here's why.</p>

<!-- more -->


<h2>Collaboration</h2>

<p>One of rubber's weaknesses is it is not a particularly great collaborative tool. If both you and someone else are provisioning a new server simultaneously, you'll get a merge conflict in your server yaml file: you really don't want to make a mistake resolving <em>that</em> merge conflict.</p>

<p>By contrast, it's really easy for multiple people to work together in Chef. You can be working in the same cookbook, even, and just altering different recipes. Bootstrapping several servers simultaneously couldn't be easier. And treating the Chef server as the central authority for cookbooks is also extremely helpful for keeping everyone on the same page with regards to what's actually going into the servers.</p>

<p>Chef is just a better tool for teams of people.</p>

<h2>Extensibility &amp; Community Support</h2>

<p>For the tools that rubber provides in its stack -- and it provides a lot -- it's an excellent solution. But adding additional facilities into rubber is a pain. You either have to come up with recipes on your own, or hope that someone has a semi-active fork with what you want in it. There's no real extensibility, and while it's easy enough to roll your own recipes, it'd definitely be ideal not to repeat work if you're fairly confident someone else has already done it.</p>

<p>Enter Chef cookbooks. There are a frightening amount of active cookbooks on Github for every need imaginable. Many are actively supported, and even if they're not precisely what you're looking for, they provide an excellent jumping-off point for creating your own solutions.</p>

<p>We're using the excellent <a href="https://github.com/applicationsonline/librarian">librarian</a> gem to manage our external cookbooks and the source cookbooks I've been developing internally for us. It's a great way to treat cookbooks like any other dependency to resolve, and will save you a lot of time in git cloning repositories.</p>

<h2>More Granularity</h2>

<p>rubber allows you to control a lot, on a per-server basis. But it has no real equivalent to data bags or even environments. Adding a user's SSH key to my deploy recipe used to be an unpleasant process. Now I can just update the users data bag with a new entry and instruct my servers to pull it: tada, new user on the servers.</p>

<p>Similarly changing postfix configuration on a per-environment basis is a snap.</p>

<h1>But Rubber is a Great Tool</h1>

<p>Don't get me wrong: I still really like rubber. It doesn't fit for Everest's use case, definitely -- with so many servers and so much going on behind the scenes, we really needed more granularity, control, and power. But if I were provisioning just one server, or even three or four, then rubber would still be my go-to tool.</p>

<p>Why? It's just a whole lot faster to get started with than Chef. It makes tons of sensible default decisions that simplify your life really significantly. You don't have to go searching for good recipes or the right way to do things. Just like Rails, rubber <strong>knows</strong> the right way to do things. As long as you take its advice you'll go far, but trying to work against its defaults will be really painful.</p>

<h2>Final Word on Chef vs. Puppet</h2>

<p>Doesn't matter, choose whichever you like more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Love/Hate Relationship with Heroku]]></title>
    <link href="http://joshsymonds.com/blog/2012/06/03/my-love-slash-hate-relationship-with-heroku/"/>
    <updated>2012-06-03T22:43:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/06/03/my-love-slash-hate-relationship-with-heroku</id>
    <content type="html"><![CDATA[<p>There's been <a href="http://justcramer.com/2012/06/02/the-cloud-is-not-for-you/">some</a> <a href="http://rdegges.com/heroku-isnt-for-idiots">discussion</a> recently about the relative merits of <a href="http://heroku.com">Heroku</a>. I've managed applications both inside Heroku and outside it, and personally speaking, I just can't decide whether or not I like the web's premier platform-as-a-service or not. Every now and again I'll gnash my teeth in frustration over it, and then other times I'll sigh dreamily and think of all the wonderful things Heroku has done for me -- and indeed, for the world. So I jotted down a quick list of pros and cons that I think everyone considering Heroku should know; then, at least, you can make an informed decision as to whether or not it's right for you.</p>

<!-- more -->


<h2>Pros</h2>

<h3>Lets Me Sleep At Night</h3>

<p>I get paged from Hipstaweb servers every now and again at odd hours, and usually for horribly arcane reasons -- an AWS instance became unresponsive, or a logfile that I thought was getting truncated suddenly spiralled out of control, or all the other small things that make systems administration so interesting. Heroku has never, ever had a stability problem like that. I imagine that sometimes stuff like that happens, but when it does Heroku just kills the dead or dying dyno behind the scenes and starts up a new one.</p>

<p>Heroku's Postgres instances also do automatic backups and have always been available whenever I've needed them. Heroku doesn't pay their infrastructure team enough, in my opinion; I've seriously never had a single outage of any sort on their service, and I have this comforting belief that if something did go horribly awry, I would be in good hands.</p>

<h3>Easy &amp; Fast</h3>

<p>Getting started on Heroku is ridiculously easy. You can have an app deployed there in just a minute or two, all from doing nothing more than adding a git remote and pushing to it. No matter how good your Chef recipes are, it'll take you at least fifteen minutes to get a bare EC2 instance provisioned from nothing to accepting HTTP connections. After you do that, of course, clever use of AMIs will get there much quicker... but still nowhere near as fast as Heroku.</p>

<h3>Extensible</h3>

<p>This is sort of a rider to "easy &amp; fast," but I think it deserves its own bulletpoint. Heroku addons provide managed solutions for nearly every need your application might have. Email sending, Redis instances, elasticsearch searching... and it's really cool of Heroku to essentially provide marketing for these smaller SaaS companies. Additionally all of the addons I've used have worked really well, so there's definitely some quality control going on.</p>

<h3>Flexible</h3>

<p>Heroku's Cedar stack is really cool. Not only can you run whatever server software you want on it (I use unicorn), but you can spin up Resque workers, rapnd daemons -- essentially anything at all, as long as you're willing to pay for the dyno it runs on. The flexibility Heroku provides allows you to run nearly anything in their managed environment, and then scale it however you find appropriate. With correct separation of concerns, this provides you an enormous amount of control over how your application is deployed and how you can respond to traffic influxes.</p>

<h2>Cons</h2>

<h3>Expensive</h3>

<p>There's really no getting around this one. SmashingBoxes wrote <a href="http://smashingboxes.com/heroku-vs-amazon-web-services/">a cost comparison</a> between Heroku and AWS, and their conclusion is inescapable: Heroku is costly. And it's not just Heroku -- especially once you start throwing in addons, your credit card will start hurting hard. RedisToGo is very pricey, and the Postgres database options are seriously expensive. Heroku provides quite a lot, so their cost understandable, but it is definitely a cost... and a big one, at that.</p>

<h3>Deploys Suck</h3>

<p>Deploying to Heroku is easy and fast -- the first time. But then when you compare the speed and ease of ongoing deployments to your own servers you start scratching your head. Compiling a slug takes a while. After slug compilation is complete, your dynos must restart: and during the restart your application is completely offline. There's no unicorn-style rolling deployments here. For small applications this is somewhat acceptable -- in a bare Rails application, my dynos restarted in 300-400 milliseconds. But in a big application you can be offline for excrucating, horrifying seconds, and that really sucks.</p>

<h3>Addon Constrained</h3>

<p>If you want to do something really customized or compiled on Heroku -- something that needs the JRE, for example -- you're pretty much screwed unless an addon already exists for it. You can't compile elasticsearch or Lucene yourself, and if you don't like the addons that provide those solutions you're essentially out of luck. You'll need to spool up your own EC2 instance, add Heroku's security group, and connect your application to it. But then you have to manage that EC2 instance yourself. And if you're doing that, why not just manage the entire application yourself as well?</p>

<h2>Conclusions</h2>

<p>All this boils down to pretty much one thing: is your app going to be big, or small? Small apps that require few dynos and no add-ons are extremely cost efficient and benefit from Heroku's platform-as-a-service. They're fast to deploy because they don't take as long to compile, and the dynos serving them restart more quickly But bigger addons will be more expensive -- and when you restart them, they take awhile to come back up. Also as an application grows your need for customized software will grow as well. Say you need hand-compiled elasticsearch or something like that; that's just something that Heroku can't provide.</p>

<p>So, for a big application, I would stick to doing it by hand. Yes, you'll have a few more infrastructure annoyances, but you'll need the control.</p>

<p>But! All big appliations started small. There's nothing preventing you from starting on Heroku and then migrating to some other solution when you start hitting stumbling blocks. Migrating data can be frustrating, but if you need Heroku's ability to start lean and quick, then don't be afraid to go for it. Just always have an eye on your exit strategy, so that when you need to do something yourself, it's easy and fast. And that's the real beauty of Heroku -- it gives you speed when you need it, and puts you in a pretty good position to graduate to something else when you outgrow it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reducing Our AWS Costs by 60%]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/12/reducing-our-aws-costs-by-60-percent/"/>
    <updated>2012-04-12T13:30:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/12/reducing-our-aws-costs-by-60-percent</id>
    <content type="html"><![CDATA[<p>Hipstamatic's Rails application is deployed to Amazon's Elastic Cloud, and we make extensive use of Amazon's Web Services in keeping it nimble and performant. Last month, I dedicated two weeks to increasing the responsiveness of the application while simultaneously improving its performance. As a result of the changes I implemented, our AWS costs for this month will be 60% lower than they were last month. This is a pretty dramatic drop, and I wanted to discuss the tools and techniques I used to make it happen.</p>

<!-- more -->


<h2>Improve Database Performance</h2>

<p>One of the biggest cost savings I implemented was scaling back our RDS instance. We use a multi-AZ deployment to ensure constant availability; unfortunately, multi-AZ instances are extremely expensive, and I knew that if we could decrease the size of our instance we could save quite a bit of money. Targeting this part of our infrastructure proved very fruitful for lowering costs, and here's what I did to decrease load on our database:</p>

<h3>Find Slow Queries</h3>

<p>Step through everything your app does with <a href="https://github.com/flyerhzm/bullet">bullet</a> and Rails 3.2's <a href="http://weblog.rubyonrails.org/2011/12/6/what-s-new-in-edge-rails-explain/">slow query explainer</a>. Really get into the nitty-gritty here: run your resque jobs, go to every single controller action, and run all the model code you can get your hands on. Make sure you have a lot of data loaded into your database when you do this, or else queries that might be slow won't show up since they aren't running under actual load conditions. I added hundreds of thousands of records using FactoryGirl like this (from the Rails console):</p>

<p><code>ruby
require 'factory_girl'
require '.test/factories.rb' # or wherever your factory file is located
200000.times do
  Factory.create(:photo)
end
</code></p>

<p>Repeat as needed. Remember your production system will likely have millions of records; get as close as you can to this without overwhelming your development machine.</p>

<h3>Index Lots</h3>

<p>Index the crap out of everything. It's nearly impossible to have too many indexes in a relational database, and every field you execute a query on should be indexed. Usually under production load conditions you'll rapidly discover that various fields that aren't indexed together should be, but examining the individual queries your application makes and ensuring they're all indexed is an awesome (though boring) use of your time.</p>

<p>I used <a href="https://github.com/plentz/lol_dba">lol_dba</a> to get a starting point for indexes I wanted to add, but honestly you'll have to get into your code yourself to find out what really needs indexing. Automated tools can't really replace actual hands-on experience... at least, not in this case.</p>

<h3>Don't Use Your Database</h3>

<p>Sticking something into the database isn't always the right solution to a problem. I discussed <a href="http://joshsymonds.com/blog/2012/03/25/elasticsearch-and-percolation-in-rails/">in a previous post</a> the problems we encountered in implementing a database-driven solution for something that should never have touched the database. Extremely large join tables in particular have awful performance, and the indexes on them can rapidly grow to a really ridiculous size. Before putting something in the database, consider if there isn't another tool to do that job. In particular, think about:</p>

<ul>
<li><h4>memcached</h4>

<p>If you want to rapidly retrieve data and it's in key-value form, and persistence doesn't really matter, use memcached instead. Be severe when you consider if something needs persistence. Do you really need to keep <em>every</em> message you pass to a client, or would keeping a count of them be sufficient?</p></li>
<li><h4>redis</h4>

<p>Redis gives you the benefits of a semi-persistent datastore with some really nice data structures. If you need lists, sets, or ordered sets -- especially if these data structures are going to end up being extremely large or called very frequently -- use redis.</p></li>
<li><h4>elasticsearch</h4>

<p>For geospatial, filter-based, and/or full-text indexing, relational database performance has nothing on dedicated indexing tools. I can't say enough nice things about elasticsearch in general and <a href="https://github.com/karmi/tire">tire</a> in particular. elasticsearch is easy to set up, has a fraction of the overhead of a database, and several times its speed. If you're performing a complicated, variated SQL query, consider if that query could be run on an indexing engine instead.</p></li>
</ul>


<h2>Use More Caching</h2>

<p>I touched on this in <a href="http://joshsymonds.com/blog/2012/04/06/how-i-scaled-hipstamatic/">my scaling post</a> but I want to restate it here: caching allows you to reduce load on every part of your application. (Except the cache I guess...) With proper caching you can remove web servers, application servers, and database servers from your setup. In addition to scaling down our database instance, we removed two entire extra-large EC2 instances because of better caching.</p>

<p>Figure out what to cache first and foremost by investigating your metrics. New Relic, Google analytics, even munin and monit will all provide you clues as to where users are going. I'd be willing to bet money almost all of your traffic is directed to the same five or ten extremely popular sources. Extract partials from those pages, or just cache them in their entirety: then serve the results instead of hitting your database (or ideally even before hitting your application servers).</p>

<p>The most important key to our caching strategy are definitely Rails sweepers. Rails sweepers keep caching DRY: instead of expiring caches manually over and over in your models and controllers, do it in one centralized place. Just keep in mind that sometimes Rails' helper methods won't find the proper cache, especially if you use multiple domains for one application. In that case specify the fragment you need to expire directly, like so:</p>

<p>```ruby
def after_save(object)
  [:domain1, :domain2, :domain3].each do |domain|</p>

<pre><code>expire_fragment "views/#{domain}/objects/#{object.id}"
</code></pre>

<p>  end
end
```</p>

<p>I have this extracted out on a per-environment basis, specifying the domains to expire in our environment files. It works out really well.</p>

<h2>Be Responsive</h2>

<p>One of the great things about being deployed to the cloud is that you can -- and should! -- scale up and down frequently. I usually have fifteen stopped EC2 instances sitting around, waiting to be added to my stack: with only a quick bootup, they'll be available to handle web or application traffic, or even add more resque workers and extra redis or memcached instances. These tools are highly dynamic and most of them can be scaled up and down quite easily, and stopped EC2 instances cost you nearly nothing (as long as the provisioned EBS drive is relatively small). And their cost is really very affordable when you consider how agile they allow you to be.</p>

<p>The key to being responsive is to communicate with your business. Is there a press release dropping that day? Time to scale up. New product coming soon? Bring those application servers online. Monitor your metrics carefully when you're at heightened capacity; when it looks like traffic has slowed, feel free to scale back down. But always be wary -- getting featured in a big publication can crush your servers unexpectedly. Ensure that you either have automated tools or great alerts letting you know when you're getting hammered, and don't be afraid to bring more servers online in a hurry. Overscaling in the short-term is a great idea: even the biggest EC2 instances cost only a few dollars an hour, and the peace of mind they give you is priceless.</p>

<p>Using these methods me allowed me to streamline our stack really significantly. Not only are we faster than we were just three months ago; we're saving a boatload of money each month. And being more awesome while spending less money is definitely a win/win.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Achieving 100% Uptime]]></title>
    <link href="http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime/"/>
    <updated>2012-04-09T17:52:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2012/04/09/achieving-100-percent-uptime</id>
    <content type="html"><![CDATA[<p><img src="http://f.cl.ly/items/0q2M3B2o1f1D1D451B1S/uptime.jpg" alt="Uptime - 100%" /></p>

<p>Keeping a highly available web application online is no joke. Everything above 99% is extremely impressive; that means that you battled the forces of <a href="http://en.wikipedia.org/wiki/Software_erosion">erosion</a> and probably even deployed some pretty neat features without even a hiccup from your users' perspective. I always feel great when I get our weekly <a href="http://newrelic.com/">New Relic</a> status report email -- it's a good indication of how well I did my job in the previous week. And for a couple weeks now I'm happy to report I've been very proud indeed, with 100% uptime on the Hipstamatic web application.</p>

<p>How do you achieve numbers like these? Unfortunately getting to 100% isn't an easy road, and I want to state up front that I also don't think it's a realistic goal. Issues you can't control can ruin your uptime number, and you shouldn't feel broken up about that. It happens to everybody. But it's always good setting goals that are difficult to achieve, and this one is no different.</p>

<p>So what's the secret to 100% uptime?</p>

<!-- more -->


<h2>Watch It Constantly</h2>

<p>Some people check their fantasy baseball league or their portfolio every morning. At the slightest hint of trouble, they'll be waist-deep in trading players or stocks to get everything right back on track. You should be that way with your servers and the software that runs on them. This usually means monitoring software, and a lot of it.</p>

<p>At Hipstamatic, we make extensive use of New Relic to give us a broad overview of our application. It helps us proactively fix nascent problems, analyzing slow queries and sluggish pages. But you need something closer to the metal, and for that we use <a href="http://mmonit.com/monit/">monit</a>. Monit is an amazing tool to control your applications' behavior and warn you when that behavior becomes dangerous. Here's a sample of our unicorn monit file:</p>

<p><code>bash
  if totalmem &gt; 70% for 4 cycles then alert
  if totalmem &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
  if cpu &gt; 70% for 4 cycles then alert
  if cpu &gt; 90% for 6 cycles then exec "kill -USR2 `cat /pids/unicorn.pid`"
</code></p>

<p>This states that I get an alert when unicorn's total memory or CPU usage exceeds 70%, and that unicorn receives a USR2 signal when total memory or CPU exceed 90%.</p>

<p>Finally, we employ <a href="http://munin-monitoring.org/">munin</a> to compile statistics that we care about, including nginx connections and unicorn requests served.</p>

<p>Yes, this is a lot of monitoring. But I feel like even this isn't enough. You can't watch your stack too carefully, and you can't have too many tools in place to help you analyze what's going on. Consider this trifecta of tools only a start, but at least it's a good one.</p>

<h2>Seamless Deploys</h2>

<p>On an average week I deploy seven to ten times. Of course, this entire process is invisible to our users; the magic that makes this happen is <a href="http://unicorn.bogomips.org/">unicorn</a>. There have been many posts on the wonders of unicorn and how to configure it correctly. I will simply post the part of our <code>unicorn.rb</code> that allows us to do seamless restarting, which you can find in a number of gists essentially unmodified.</p>

<p>```ruby
before_fork do |server, worker|
  old_pid = "#{server.config[:pid]}.oldbin"
  if File.exists?(old_pid) &amp;&amp; server.pid != old_pid</p>

<pre><code>begin
  Process.kill("QUIT", File.read(old_pid).to_i)
rescue Errno::ENOENT, Errno::ESRCH
end
</code></pre>

<p>  end
end
```</p>

<p>The command we use to restart unicorn is:</p>

<p><code>bash
  if [ ! -f '/pids/unicorn.pid' ]; then cd current_path &amp;&amp; bundle exec unicorn_rails -c ./config/unicorn.rb -E production -D; else kill -USR2 `cat /pids/unicorn.pid`; fi
</code></p>

<p>USR2 is the signal that tells unicorn to start reloading itself: the before_fork causes the new server to kill the old server only when it's ready to start processing connections.</p>

<h2>Migrations Without Downtime</h2>

<p>The last key component to 100% uptime is migrating your database without bringing your site down. Of course, this only applies if you're changing how existing code interacts with the database -- for new tables, simply migrate before deploying and you're done. If only it could be that easy all the time...</p>

<p>Frequently we are required to change existing tables and colums or add new ones. For those of us still using relational databases, migrations almost always mean locked tables, and locked tables mean site downtime. To fix this problem, my tool of choice has been <a href="https://github.com/soundcloud/large-hadron-migrator">Large Hadron Migrator</a>. Large Hadron Migrator requires very little from your tables (just an autoincrementing ID) and allows you to alter tables and even add new columns without bringing your site down.</p>

<p>```ruby
class AddOrdersCountToUsers &lt; ActiveRecord::Migration</p>

<p>  def self.up</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} ADD COLUMN orders_count INT(11) default 0")
end
</code></pre>

<p>  end</p>

<p>  def self.down</p>

<pre><code>Lhm.change_table(:users) do |m|
  m.ddl("ALTER TABLE #{m.name} DROP COLUMN orders_count")
end
</code></pre>

<p>  end</p>

<p>end
```</p>

<p>Yes, unfortunately, this includes raw SQL. There is a <a href="http://rubydoc.info/github/soundcloud/large-hadron-migrator/master/Lhm/Migrator#add_column-instance_method">small DSL</a> that exposes a few common methods, but for anything really deep you're gonna need to get your hands dirty. Using this method you'll be able to become the envy of your friends and peers, for you'll be able to execute zero downtime migrations.</p>

<p>And those three points are the main ways I've reduced our downtime. It's a difficult road to 100%, but it's worth it because you can stare at pretty graphs like this:</p>

<p><img src="http://f.cl.ly/items/470B350J0U0q1u3r0T0s/availability-1.jpg" alt="Better than Facebook" /></p>

<p>And imagine that your website and a 100% bar are sitting right at the very tippy top.</p>
]]></content>
  </entry>
  
</feed>
