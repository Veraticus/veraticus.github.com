<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: servers | Hi, I'm Josh Symonds]]></title>
  <link href="http://joshsymonds.com/blog/categories/servers/atom.xml" rel="self"/>
  <link href="http://joshsymonds.com/"/>
  <updated>2015-06-10T11:22:57-05:00</updated>
  <id>http://joshsymonds.com/</id>
  <author>
    <name><![CDATA[Josh Symonds]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Packaging Anything with Chef and fpm]]></title>
    <link href="http://joshsymonds.com/blog/2015/06/10/packaging-anything-with-chef-and-fpm/"/>
    <updated>2015-06-10T10:33:45-05:00</updated>
    <id>http://joshsymonds.com/blog/2015/06/10/packaging-anything-with-chef-and-fpm</id>
    <content type="html"><![CDATA[<p>Compiling software takes a long time. The worst offender, for us, is usually Ruby, but it could be anything &ndash; recently we had a client that wanted to install ffmpeg on each server, with compilation times upwards of five minutes. The answer to getting around long compilation times in a standardized server environment is, of course, packages! So we developed a Chef-based solution to create packages with the really excellent <a href="https://github.com/jordansissel/fpm">fpm</a>, upload them to S3, and then download them on target servers: all without needing anything other than a few gems.</p>

<p>Want to do it yourself? Then read on.</p>

<!-- more -->


<h2>Creating Packages</h2>

<p>There are two parts to this setup: first, compiling the software and creating packages from it. Then, downloading it on client servers. For the purposes of this cookbook, I&rsquo;ll be referring to the former as &ldquo;creating,&rdquo; and the latter as &ldquo;installing.&rdquo;</p>

<p>Creating is pretty easy. We&rsquo;ll have one generic <code>create</code> recipe that all the other recipes can include, and a provider that does most of the heavy lifting. First, let&rsquo;s set up some cookbook attributes:</p>

<pre><code class="ruby"># symondsandson_packages/attributes/default.rb

# Compile extensions immediately
default['build-essential']['compile_time'] = true
default['xml']['compiletime']              = true

# fpm settings
default['symondsandson_packages']['fpm']['version'] = '1.3.3'
set['fpm_tng']['exec']                       = '/opt/chef/embedded/bin/fpm'
set['fpm_tng']['gem']                        = '/opt/chef/embedded/bin/gem'

# S3 settings
default['symondsandson_packages']['s3']['bucket']            = 'packages'
default['symondsandson_packages']['s3']['download_path']     = 'production' # By default, download from production
default['symondsandson_packages']['s3']['upload_path']       = 'development'  # By default, upload to development
default['symondsandson_packages']['s3']['access_key_id']     = 'XXX'
default['symondsandson_packages']['s3']['secret_access_key'] = 'YYY'

# Download settings
default['symondsandson_packages']['download']['cache_directory'] = '/usr/local/symondsandson/cache/'
</code></pre>

<p>I&rsquo;ve pegged our package creation to fpm 1.3.3, which has worked extremely well for us. Remember to include your actual access key and secret access key to actually upload this to S3! Also note the cache directory location. We&rsquo;ll be using that down below.</p>

<p>Now the base create recipe:</p>

<pre><code class="ruby"># symondsandson_packages/recipes/create.rb

# Install everything we need to create packages.
include_recipe 'apt'
include_recipe 'build-essential'
include_recipe 'xml'

package "zlib-devel compiletime install wget liblzma-dev libssl-dev libyaml-dev libreadline6-dev"

chef_gem 'fpm' do
  version node['symondsandson_packages']['fpm']['version']
  action :upgrade
end

chef_gem 'fog' do
  version '1.25.0'
end
</code></pre>

<p>Here we&rsquo;re just installing a bunch of packages that will be necessary for compiling whatever software we choose. And finally, let&rsquo;s create a version of Ruby:</p>

<pre><code class="ruby"># symondsandson_packages/recipes/create_ruby.rb

# Pull in package creation prerequisites.
include_recipe 'symondsandson_packages::create'

# Pull in Ruby prerequisites
%w(libffi6 libffi-dev).each do |package|
  apt_package package
end

# Install rbenv
include_recipe 'custom_ruby::rbenv'

rbenv_ruby '2.2.0' do
  environment({
    'CONFIGURE_OPTS' =&gt; "--enable-shared --with-opt-dir=/usr/local/rbenv/versions/2.2.0"
  })
end

%w(bundler backup puma nokogiri).each do |g|
  rbenv_gem g do
    rbenv_version '2.2.0'
    action :install
  end
end

symondsandson_packages 'ruby' do
  version '2.2.0'
  input_args '.'
  prefix "/usr/local/rbenv/versions/2.2.0"
  chdir "/usr/local/rbenv/versions/2.2.0"
  action :create
end
</code></pre>

<p>Again, we&rsquo;re just installing package prerequisites, compiling Ruby and a few gems, and then using a provider called <code>symondsandson_packages</code> to do the real work. The provider is quite special, so let&rsquo;s dig into it in a little bit more detail.</p>

<h2>The Package Provider</h2>

<p>Of course, the package provider does most of the heavy lifting. In addition to creating the packages, it also supports installing them. Before we get to it though, let&rsquo;s implement a little bit of abstraction so we can DRY up the provider a bit:</p>

<pre><code class="ruby"># symondsandson_packages/libraries/packages_helper.rb

# A few DRY helpers.
module SymondsandsonPackages
  extend self

  def name(node, package, version)
    "#{package}-#{version}.deb"
  end

  def download_path(node, package, version)
    File.join(node['symondsandson_packages']['s3']['download_path'], package, self.name(node, package, version))
  end

  def upload_path(node, package, version)
    File.join(node['symondsandson_packages']['s3']['upload_path'], package, self.name(node, package, version))
  end

end
</code></pre>

<p>This shouldn&rsquo;t exactly be surprising stuff, and we&rsquo;re about to use it extensively!</p>

<pre><code class="ruby"># symondsandson_packages/providers/default.rb

require 'chef/mixin/shell_out'
require 'chef/mixin/language'
include Chef::Mixin::ShellOut

use_inline_resources

def load_current_resource
  @package_name = ::SymondsandsonPackages.name(node, new_resource.name, new_resource.version)
  @download_path = ::SymondsandsonPackages.download_path(node, new_resource.name, new_resource.version)
  @upload_path = ::SymondsandsonPackages.upload_path(node, new_resource.name, new_resource.version)
  @cache_directory = new_resource.cache_directory || node['symondsandson_packages']['download']['cache_directory']
end

action :install do
  run_context.include_recipe 'apt'
  run_context.include_recipe 'symondsandson_packages::default'
  run_context.include_recipe 's3_file'

  s = s3_file(::File.join(@cache_directory, @package_name)) do
    bucket node['symondsandson_packages']['s3']['bucket']
    aws_access_key_id node['symondsandson_packages']['s3']['access_key_id']
    aws_secret_access_key node['symondsandson_packages']['s3']['secret_access_key']
    action :nothing
  end
  s.instance_variable_set(:@remote_path, "/#{@download_path}") # Instance variables do not enter a lwrp setting block
  s.run_action(:create)

  d = dpkg_package(@package_name) do
    action :nothing
  end
  d.instance_variable_set(:@source, ::File.join(@cache_directory, @package_name)) # Instance variables still do not enter a lwrp setting block
  d.run_action(:install)

  new_resource.updated_by_last_action(s.updated_by_last_action? &amp;&amp; d.updated_by_last_action?)
end

action :create do
  run_context.include_recipe 'fpm-tng::default'

  fpm_tng_package new_resource.name do
    input_type 'dir'
    output_type 'deb'
    version new_resource.version
    prefix new_resource.prefix
    chdir new_resource.chdir
    input_args new_resource.input_args
    provides new_resource.provides
  end

  ruby_block "upload #{new_resource.name} package" do
    block do
      require 'fog'

      connection = Fog::Storage.new provider:              'AWS',
                                    aws_access_key_id:     node['symondsandson_packages']['s3']['access_key_id'],
                                    aws_secret_access_key: node['symondsandson_packages']['s3']['secret_access_key']

      bucket = connection.directories.get node['symondsandson_packages']['s3']['bucket']

      file = bucket.files.create key:  ::SymondsandsonPackages.upload_path(node, new_resource.name, new_resource.version),
                                 body: ::File.open("/opt/fpm-pkgs/#{new_resource.name}-#{new_resource.version}.deb")
    end
  end

end
</code></pre>

<p>There&rsquo;s a lot of stuff going on here, so let&rsquo;s take it piece by piece.</p>

<p>First, we set some instance variables for use throughout the provider. We use them almost immediately in the <code>install</code> action with the excellent <a href="https://github.com/adamsb6/s3_file">s3_file</a> Chef provider, which allows us to easily download files from S3. Tragically we are forced to do some Ruby gymnastics to set instance variables for the providers, since instance variables do not enter blocks appropriately in Chef&hellip; but once the file is downloaded from S3, we use dkpg_package on it to install it. If both the files was downloaded and installed, then the <code>install</code> action updated its resource.</p>

<p>The <code>create</code> action is just as easy. We pass in a bunch of arguments that fpm expects: here we make extensive use of the <a href="https://github.com/hw-cookbooks/fpm-tng">fpm_tng</a> provider, which wraps the installation and use of fpm. Finally, we manually upload the created package file using Fog directly.</p>

<p>Using this provider and the <code>create</code> recipes above, you should be able to start up a vagrant or test-kitchen instance using a recipe like <code>symondsandson_packages::create_ruby</code> and have it automatically compile and upload Ruby to an S3 bucket of your choosing.</p>

<h2>Installing Packages</h2>

<p>Now that we have the provider out of the way, installing packages is simplicity itself!</p>

<pre><code class="ruby"># symondsandson_packages/recipes/install_ruby.rb

# Do common basics.
include_recipe 'apt'
include_recipe 'build-essential'

# Install necessary packages
package "zlib-devel compiletime install" do
  package_name 'zlib1g-dev'
end.run_action(:install)

# Install rbenv to manage the Ruby versions
include_recipe 'custom_ruby::rbenv'

# Install libyaml so Ruby can function
package 'libyaml-dev'

symondsandson_packages "ruby-2.2.0" do
  name 'ruby'
  version '2.2.0'
  action :install
end

# Make sure rbenv detects and sets shims for this Ruby
rbenv_global '2.2.0'

end
</code></pre>

<p>This simple and robust system has allowed us to package up any kind of software in a repeatable, efficient manner and then deploy it to multiple servers quickly. It&rsquo;s saved our clients tons of time wasted in compilation, and was pretty interesting and fun to code besides. Hopefully you&rsquo;ll find it useful as well!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Chef Cookbook Continuous Integration]]></title>
    <link href="http://joshsymonds.com/blog/2015/02/04/chef-cookbook-continuous-integration/"/>
    <updated>2015-02-04T11:37:38-06:00</updated>
    <id>http://joshsymonds.com/blog/2015/02/04/chef-cookbook-continuous-integration</id>
    <content type="html"><![CDATA[<p>Testing infrastructure is as crucial to the success of a business as testing applications. Yet most infrastructure is untested and validated only occasionally, and only by hand &ndash; this is especially a tragedy when chef is used, because chef has many high-quality tools to provide testing coverage from <a href="https://foodcritic.io">static analysis</a> to <a href="https://github.com/sethvargo/chefspec">unit testing</a> and even <a href="http://kitchen.ci/">full convergence runs</a>.</p>

<p>At Symonds &amp; Son, we spent a lot of time and energy integrating our tests on <a href="https://circleci.com">CircleCI</a> into a continuous deployment process that begins with three layers of testing and ends with automated cookbook deployment to the chef servers we manage. I&rsquo;ll discuss chef continuous deployment in a later article; here, I&rsquo;ll cover how we got Foodcritic, ChefSpec, and Test Kitchen all running seamlessly in Circle.</p>

<!-- more -->


<blockquote><p>My good friend and coworker, <a href="https://twitter.com/chrislopresto">Chris LoPresto</a>, contributed greatly to the engineering innovations discussed here.</p></blockquote>

<p>CircleCI has a number of great integrations: here we&rsquo;ll use their docker service to create docker instances and automatically converge them with Test Kitchen. Before we do so, we&rsquo;ll run foodcritic and ChefSpec on them as well just to make sure everything works as we would expect.</p>

<h2>Get Tested</h2>

<p>You&rsquo;ll need to actually set up tests and get them running before getting them into continuous integration! Happily the process of testing cookbooks is relatively fast and easy: all three software tools Symonds &amp; Son uses have great documentation and tons of examples on the Internet. Here&rsquo;s how we set them up.</p>

<h3>Foodcritic</h3>

<p>Running Foodcritic is pretty simple. Simply add the foodcritic gem to your <code>Gemfile</code> and execute a command like this:</p>

<pre><code class="sh">foodcritic . -X spec -f any -t ~FC003
</code></pre>

<p><code>~FC003</code> instructs Foodcritic not to use a rule that guards for chef-solo. We intentionally do not obey rule FC003 as we use chef-zero locally and chef-server remotely.</p>

<h3>ChefSpec</h3>

<p>ChefSpec is slightly more complicated. Your best bet is to follow the excellent installation guide at the <a href="https://github.com/sethvargo/chefspec">ChefSpec README</a>, since we don&rsquo;t really do any customization on top of that: our ChefSpec tests are rather traditional. Here&rsquo;s a sample from our cookbook that updates aptitude:</p>

<pre><code class="ruby">require 'chefspec'
require_relative '../spec_helper'

describe 'custom_apt::default' do

  before { stub_recipes %w(apt) }

  let(:chef_run) { ChefSpec::Runner.new.converge(described_recipe) }

  it 'upgrades upstart' do
    expect(chef_run).to upgrade_package('upstart')
  end

end
</code></pre>

<h3>Test Kitchen</h3>

<p>The most complicated of the chef testing suites, Test Kitchen performs actual convergence on a platform of your choice and then will run automated tests on the created instance. Test Kitchen is also the most essential of the suites, in my opinion &ndash; performing static analysis and unit tests are all well and good, but the only way to definitively determine if your cookbook works is to actually ensure servers converge and that their internal state is correct.</p>

<p>Most of our cookbooks use <a href="https://docker.com">Docker</a> containers to perform testing. This has downsides: docker containers will not allow you to modify important files in their <code>/etc</code> directory. For example, you cannot change iptables rules in a docker container. Additionally Upstart does not work at all in docker-land. Despite drawbacks like this, docker has many advantages. It is extremely fast and well-supported across testing providers. In fact, our continuous integrator of choice, Circle, provides first-class support for docker containers on their test VMs &ndash; allowing you to run docker Test Kitchen convergences directly on Circle.</p>

<p>Happily, the <code>.kitchen.yml</code> that supports this is rather standard:</p>

<pre><code class="yaml">---
driver:
  name: docker
  privileged: true

provisioner:
  name: chef_zero
  require_chef_omnibus: 11.16.4
  attributes:
    test-kitchen: true

platforms:
  - name: ubuntu-14.04

suites:
  - name: default
    run_list:
      - recipe[cookbook::default]
</code></pre>

<p>The only special section to note here is the driver configuration: we&rsquo;re using docker and setting it to privileged mode, which enables it to more exactly match a cloud VM.</p>

<p>Just for completeness&#8217; sake, here&rsquo;s a simple test from the afore-mentioned aptitude cookbook testing for the proper version of Upstart:</p>

<pre><code class="ruby">require 'serverspec'
set :backend, :exec

describe command('apt-cache policy upstart | grep Installed') do
  its(:exit_status) { should eq 0 }
  its(:stdout) { should include('1.12.1') }
end
</code></pre>

<p>Once you have your three testing suites set up, all that remains is to integrate them into Circle.</p>

<h2>Circle Continuous Integration</h2>

<p><a href="https://circleci.com">CircleCI</a> is my favorite continuous integration tool. It has a great UI and is really simple to set up with GitHub: it also has great support for third-party plugins, and even better, lets you SSH into a failed instance to run tests yourself and figure out exactly what went wrong.</p>

<p>For our purposes, we need CircleCI to properly install chef and then run all the tests on our cookbook. As it turns out this is not all that difficult to set up: you&rsquo;ll want a <code>circle.yml</code> that looks a little like this&hellip;</p>

<pre><code class="yaml">machine:
  services:
    - docker
dependencies:
  pre:
    - if ! chef -v; then
        if ! [ -f chefdk_0.3.5-1_amd64.deb ]; then
          wget https://opscode-omnibus-packages.s3.amazonaws.com/ubuntu/12.04/x86_64/chefdk_0.3.5-1_amd64.deb;
        fi;
        sudo dpkg -i chefdk_0.3.5-1_amd64.deb;
      fi
    - chef gem install specific_install
    - sudo chef gem specific_install kitchen-docker -l http://github.com/peterabbott/kitchen-docker.git -b v1.6.4
    - sudo chef gem uninstall chefspec
    - chef gem install chefspec:4.0.1
    - mkdir ~/.chef
    - cp ~/${CIRCLE_PROJECT_REPONAME}/test/circle/knife.rb ~/.chef/knife.rb
  cache_directories:
    - ./chefdk_0.3.5-1_amd64.deb
test:
  override:
    - chef exec berks install
    - chef exec rspec -P spec/**/*_spec.rb --tty --color
    - chef exec foodcritic . -X spec -f any -t ~FC003
    - chef exec kitchen test
</code></pre>

<p>There&rsquo;s a lot going on here, so let&rsquo;s dive in at the top!</p>

<p>First, the <code>machine.services</code> directive informs CircleCI we want docker to start on our Circle test VMs. Without this, the kitchen specs won&rsquo;t work at all.</p>

<p>Next comes the dependencies directive. Chef provides the excellent <a href="https://downloads.chef.io/chef-dk/">ChefDK</a> download which bundles everything we care about: chef&rsquo;s own command-line interface, <a href="https://berkshelf.com">Berkshelf</a> (for managing cookbook dependencies), and all three testing tools we&rsquo;re using. We&rsquo;re installing the official Chef <code>.deb</code> distribution of ChefDK onto the Circle machine to get around downloading each tool individually.</p>

<p>Next comes a bit of gem back-and-forth. Until very recently, the GitHub <a href="https://github.com/portertech/kitchen-docker">kitchen-docker main fork</a> was not kept up-to-date: we manually install a version of kitchen-docker that actually works, and then reinstall a better version of chefspec.</p>

<p>Finally, we copy a stub <code>knife.rb</code> to <code>~/.chef/knife.rb</code>. You&rsquo;ll need to set this file up yourself, but it&rsquo;s intentionally pretty small. If you&rsquo;re downloading any cookbooks from a Berkshelf-API server, you&rsquo;ll probably need to include a valid private key for that server either in the <code>knife.rb</code> (bad) or include it as an environment variable in Circle (good). Here&rsquo;s what the <code>knife.rb</code> should look like:</p>

<pre><code class="rb"># A knife.rb for Circle

node_name 'circleci'
chef_server_url 'https://your.chef.url'
client_key ENV['CIRCLE_CI_MACHINE_USER_CHEF_KEY']
</code></pre>

<p>Once all the setup is done, running the tests is by comparison quite simple! We perform a <code>berks install</code> and then run each test command individually. If you have your tests working locally, this should get them working properly in Circle as well.</p>

<p>Of course, even if you are testing your cookbooks, you&rsquo;re really only half-way to heaven. After testing comes deployment: and this can be a little difficult with chef&rsquo;s complicated keying and validation structure. In my next post I&rsquo;ll detail how we automatically release and deploy cookbooks to their destination chef servers&hellip; stay tuned!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Deployments with Consul]]></title>
    <link href="http://joshsymonds.com/blog/2014/10/21/continuous-deployments-with-consul/"/>
    <updated>2014-10-21T17:13:30-05:00</updated>
    <id>http://joshsymonds.com/blog/2014/10/21/continuous-deployments-with-consul</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve fallen in love with <a href="http://www.consul.io/">consul</a>. At first glance, it&rsquo;s a bit like <a href="http://zookeeper.apache.org/">zookeeper</a> or <a href="https://github.com/coreos/etcd">etcd</a> &ndash; it handles service discovery, health checking, and even features a very simple k/v store. But consul does much more than merely expose a lovely DNS interface: one of its more powerful features is its ability to do cluster orchestration, efficiently and effectively propagating messages to all nodes. One of my client projects runs on a dozen servers across two applications; here&rsquo;s how I integrated CircleCI, chef, and consul together to make any GitHub commit run a deploy only to the targeted application, without needing to know any application server&rsquo;s name or IP address.</p>

<!-- more -->


<h2>Your servers</h2>

<p>This setup assumes you have your servers connect to chef for both provisioning and deployment; additionally, that your servers are named at least partially after the applications they serve. (I find this to be good practice anyway &ndash; as cute as it is to have server names themed from Teenage Mutant Ninja Turtles, in practice you just end up getting confused about what they do.) All the servers should be able to communicate together on a secure network. That&rsquo;s what you&rsquo;ll run consul on!</p>

<h2>consul</h2>

<p>First, you need to <a href="http://www.consul.io/intro/getting-started/install.html">install consul</a> on your servers and get them <a href="http://www.consul.io/intro/getting-started/join.html">communicating together properly</a>. As you can tell from my links, the consul documentation is extremely well-equipped to get you a fully functional consul cluster: just follow it and you&rsquo;ll be fine. Once it&rsquo;s all installed, you&rsquo;ll want to drop a watch like this in your app server&rsquo;s consul config directory:</p>

<pre><code class="json">{
  "name": "run-deploy",
  "handler": "sudo /usr/bin/chef-client",
  "type": "event"
}
</code></pre>

<p>Note the sudo above: chef-client must be run as root, but this shouldn&rsquo;t be too big of a concern. Just make sure that your consul service is running as a sudoer that can execute only that one command and you&rsquo;ll be fine.</p>

<h2>CircleCI</h2>

<p>Getting CircleCI set up properly with your GitHub repository is covered in great detail <a href="https://circleci.com/docs/configuration">at CircleCI&rsquo;s documentation</a>. Make sure your tests pass on CircleCI before you continue.</p>

<p>In order for this to work, CircleCI will have to be able to propagate that event to all your servers. The easiest way for it to do this is to have it SSH into one server and run the <code>consul event</code> command. From there, consul takes over to make sure all the appropriate servers receive the message. So let&rsquo;s create an SSH key for our CircleCI user.</p>

<pre><code class="bash">ssh-keygen -t rsa -C "continuous_deployment@example.com"
</code></pre>

<p>Add the resulting private key to CircleCI and create a corresponding user on one of the central servers of your cluster. In my setup, I have three separate instances all running consul in server mode: I assigned one of them the DNS <code>consul.example.com</code> and created a user there to accept Circle&rsquo;s SSH key. Notably this user does not need any sort of permissions at all, so leave them off the sudoers.</p>

<h2>Your project</h2>

<p>Now you need to change your circle.yml in your project, so that after a build CircleCI will initiate the deploy. This is pretty simple:</p>

<pre><code class="yaml">deployment:
  production:
    branch: production
    commands:
      - ssh example@ip "consul event -node &lt;app_name&gt; -name run-deploy"
</code></pre>

<p>Note the <code>&lt;app_name&gt;</code> filter there for nodes. I find it a good practice to name a node after the application running on it; so I might have one called <code>app1-web</code> and another <code>app2-worker</code>. This is helpful for performing consul node filtering: by providing <code>-node &lt;app_name&gt;</code>, you&rsquo;re ensuring that the consul event is only propagated to servers that actually run the targeted application.</p>

<p>The next time you push a commit, CircleCI will automatically SSH into your server and execute that event. Your application servers with the watches on them will run <code>chef-client</code> and pull down the latest code, automatically deploying the most recent version of your application.</p>

<p>This is obviously a fairly simple application of consul, but I found it extremely easy to setup and drop into my existing application provisioning and deployment process. consul provides a lot of automation and power, however, and you&rsquo;ll find it perfect for helping to bridge the small gaps in your provisioning system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With AWS OpsWorks]]></title>
    <link href="http://joshsymonds.com/blog/2014/06/11/getting-started-with-aws-opsworks/"/>
    <updated>2014-06-11T13:59:00-05:00</updated>
    <id>http://joshsymonds.com/blog/2014/06/11/getting-started-with-aws-opsworks</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been creating a complicated OpsWorks server setup for a client, as I mentioned in <a href="http://joshsymonds.com/blog/2014/05/09/creating-an-aws-opsworks-instance-store-ami/">my last post</a>, and I&rsquo;ve been really enjoying the process. OpsWorks, while still a beta service, has a lot to recommend itself: it couples the best parts of chef to the power of the impressive AWS APIs. Using OpsWorks, it&rsquo;s easy to make processes that seem almost magical.</p>

<p>How magical? Well, imagine super-fast command line deploys, seamless cookbook updates, great chatbot and application integration, then marry all those things to AWS autoscaling via elastic load balancing. One use case for my client: <a href="https://travis-ci.org/">TravisCI</a> automatically creating servers, running remote acceptance tests on them, then destroying them afterwards &ndash; all while notifying chatrooms of its progress. Now that&rsquo;s assurance your code will work in production! Really, the sky&rsquo;s the limit here for awesome integrations.</p>

<p>I&rsquo;ve learned a lot in the process of implementing this setup. If you&rsquo;re looking to give OpsWorks a go for your next project, here&rsquo;s some hints and tips to make get started on the right path.</p>

<!-- more -->


<h2>1. Setup vagrant to be compatible with OpsWorks</h2>

<p>You&rsquo;ll want to test all of your OpsWorks recipes locally &ndash; how else can you be sure they&rsquo;ll work remotely? <a href="http://www.vagrantup.com/">Vagrant</a> is the ideal tool for making this happen. You&rsquo;ll want to download the same AMI and the same version of chef that OpsWorks is using: ubuntu 12.04 and chef 11.10 respectively, for me. Here&rsquo;s how to do that in your <code>Vagrantfile</code>:</p>

<pre><code class="ruby">VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "ubuntu-precise64"
  config.vm.box_url = "https://opscode-vm-bento.s3.amazonaws.com/vagrant/opscode_ubuntu-12.04_provisionerless.box"

  # Specifies the chef version Opsworks is running
  config.omnibus.chef_version = "11.10.0"
end
</code></pre>

<p>This requires the <code>vagrant-omnibus</code> plugin, which you can install with <code>vagrant plugin install vagrant-omnibus</code>.</p>

<h2>2. Use librarian-chef</h2>

<p>OpsWorks expects all of your recipes to be in one git repository that it can download. This may be bad practice for many chefs, but since it&rsquo;s required here and it&rsquo;s the cookbook repository format that <a href="https://github.com/applicationsonline/librarian-chef">librarian-chef</a> expects and supports, you&rsquo;ll want to download and configure librarian-chef.</p>

<p>Just the default librarian-chef configuration works, with one exception: you&rsquo;ll want to strip <code>.git</code> directories from the checked-out sources to prevent OpsWorks from becoming confused. That&rsquo;s relatively easy to set up:</p>

<pre><code class="bash">librarian-chef config install.strip-dot-git 1 --local
</code></pre>

<p>I store the source of my cookbooks in one repository and use an orphan branch of that same repository for the actual cookbooks that are installed and managed by librarian. This is pretty easy to set up:</p>

<pre><code class="bash"># Create the orphan branch
git checkout --orphan cookbooks
git rm -rf .
git add . -A
git commit -m 'Initial commit'
git push origin cookbooks
</code></pre>

<p>Then in your master branch, set up your cookbook branch as a submodule in a subdirectory that librarian-chef will install to:</p>

<pre><code># .gitmodules
[submodule "cookbooks"]
  path = cookbooks
  url = git@github.com:user/repository.git
  branch = cookbooks
</code></pre>

<p>I have a small Rakefile that allows me to run <code>rake</code> to sync my changes directly to the cookbooks branch.</p>

<pre><code class="ruby">desc "install all cookbooks and synchronize them to GitHub"
task :default do
  puts "## Installing cookbooks"
  system "librarian-chef install"
  puts "## Pushing cookbooks to GitHub"
  cd "cookbooks" do
    system %Q(echo "gitdir: ../.git/modules/cookbooks" &gt; .git)
    system "git add ."
    system "git add -u"
    message = "Cookbooks generated via librarian-chef at #{Time.now.utc}"
    system "git commit -m \"#{message}\""
    system "git pull"
    system "git push origin cookbooks"
  end
  puts "## Done!"
end
</code></pre>

<p>Keep in mind this setup isn&rsquo;t ideal for collaboration: if I had a lot of people updating the cookbooks simultaneously, I would definitely set up separate repositories. But for smaller OpsWorks projects, this works perfectly well.</p>

<h2>3. Don&rsquo;t bother with OpsWorks&#8217; recipe syntax</h2>

<p>Though it&rsquo;s clever that OpsWorks has their own recipe syntax they&rsquo;d like you to use, my advice is: don&rsquo;t. If you ever want to use your chef recipes somewhere else &ndash; or bring chef recipes from elsewhere to OpsWorks &ndash; you&rsquo;ll thank yourself for just using the standard recipe format. So instead of this:</p>

<pre><code class="ruby"># No!
node[:deploy].each do |app_name, deploy|
  template '/etc/init/puma.conf' do
    source 'puma.conf'
    owner  'root'
    group  'root'
    mode   '0644'
  end
end
</code></pre>

<p>Use the plainer, simpler:</p>

<pre><code class="ruby"># Yes!
template '/etc/init/puma.conf' do
  source 'puma.conf'
  owner  'root'
  group  'root'
  mode   '0644'
end
</code></pre>

<p>The former syntax won&rsquo;t work properly on vagrant, just for starters, which is a great reason all by itself not to use it. You&rsquo;ll want to control what recipes get applied where through custom layers rather than OpsWorks&#8217; special syntax.</p>

<h2>4. Overwrite any recipes that overlap</h2>

<p>OpsWorks inserts a lot of their own recipes into your cookbooks, and you can&rsquo;t disable this behavior, even if you&rsquo;re using your own custom recipes. This can lead to naming collisions that can be frustrating to resolve. For a Rails stack, I had to manually remove the <code>unicorn</code> and <code>passenger-apache2</code> cookbooks that led to merge errors with the <code>application_ruby</code> cookbook. Thankfully, removing cookbooks in OpsWorks is pretty easy: if you have a recipe named exactly the same as an OpsWorks one, yours will replace it.</p>

<p>You&rsquo;ll want to create a cookbook named after the offending cookbook (for example, <code>unicorn</code>) and replace every file in the OpsWorks cookbook with a blank one. You can find all the OpsWorks cookbook sources <a href="https://github.com/aws/opsworks-cookbooks">in their GitHub repository</a>. So, to continue the unicorn example, you&rsquo;d make a <code>unicorn</code> directory, a <code>recipes</code> subdirectory, and three files: <code>default</code>, <code>rails</code>, and <code>stop</code>. The content of all these files should be something like this:</p>

<pre><code class="ruby"># Prevent OpsWorks from trying to install this cookbook.
</code></pre>

<p>Obviously you should only do this if you&rsquo;re definitely not using OpsWorks&#8217; cookbooks.</p>

<h2>5. OpsWorks is your single point of truth</h2>

<p>Get rid of your data bags, encrypted data bags, configuration yaml files: everything. Embrace OpsWorks as your centralized chef server and the primary authority on the state and setup of your application. Data bags are arguably chef smell at this point anyway, and OpsWorks continues their inexorable slide towards obsolescence. You&rsquo;ll want to set up everything you can with sensible attributes in your custom application recipes:</p>

<pre><code class="ruby"># site-cookbooks/your-app/attributes/default.rb
default['database'] = {
  'pool' =&gt; 5,
  'host' =&gt; 'localhost',
  'name' =&gt; 'app_database',
  'username' =&gt; 'username',
  'password' =&gt; 'password'
}
</code></pre>

<p>Then pass overrides in your stack JSON. Your stack JSON is where you&rsquo;ll enumerate all the settings particular to your environment: though I&rsquo;m not incredibly happy with this setup, as it&rsquo;s not versioned, AWS makes it easy to copy stack and layer setups really easily, so in practice it&rsquo;s not difficult to update multiple stacks or create a new one from sensible defaults.</p>

<h2>6. Use the AWS API</h2>

<p>So what&rsquo;s the real advantage of doing this whole song and dance? Using the AWS API, you can command and control your servers (and all your attached AWS stuff) with an ease and simplicity you can&rsquo;t achieve anywhere else. But for more details on that, you&rsquo;ll just have to stay tuned for my next post, which will discuss all the awesome things you can start doing with OpsWorks once you have it set up properly.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating an AWS OpsWorks Instance Store AMI]]></title>
    <link href="http://joshsymonds.com/blog/2014/05/09/creating-an-aws-opsworks-instance-store-ami/"/>
    <updated>2014-05-09T13:00:45-05:00</updated>
    <id>http://joshsymonds.com/blog/2014/05/09/creating-an-aws-opsworks-instance-store-ami</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been doing a fair amount of work in Amazon&rsquo;s <a href="https://aws.amazon.com/opsworks/">OpsWorks</a>, in many ways an elegant service. Once you have a set of chef recipes provisioning properly, you&rsquo;ll want to create an AMI for the layer in question so that you don&rsquo;t have to wait through a long setup process again. Unfortunately, doing this in OpsWorks can be frustrating since the instructions for making it happen are scattered across <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-custom-ami.html">four</a> <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-instance-store.html">entirely</a> <a href="http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/ec2-cli-get-set-up.html#install-ami-tools">different</a> <a href="http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-linux.html">documents</a>. For my own sanity I made a checklist of all the steps necessary to create an AMI on an instance store EC2 server: this is that checklist for anyone else who might find it useful.</p>

<!-- more -->


<h2>0. Get your chef recipes working in OpsWorks</h2>

<p>Don&rsquo;t do any of this until you have a fully-provisioned server working exactly as you&rsquo;d expect. Make sure <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html">auto healing</a> is disabled for the layer. The rest of the steps assume you have such a properly set-up instance, a well configured layer, and that the original image was Ubuntu 12.04.</p>

<h2>1. Download your X.509 certificates</h2>

<p>Amazon has a pretty good <a href="http://docs.aws.amazon.com/AmazonDevPay/latest/DevPayDeveloperGuide/X509Certificates.html">checklist</a> for how to do this. You need X.509 certificates so only you and Amazon can access your AMI, which for most layers is a sensible security precaution. For this walkthrough I&rsquo;ll assume you have the private key downloaded to ~/certs/pk-X509.pem and the the certificate downloaded to ~/certs/cert-X509.pem.</p>

<h2>2. Transfer your certs to the server</h2>

<p>On the server itself:</p>

<pre><code class="bash">mkdir -p /tmp/cert/
</code></pre>

<p>On your local computer:</p>

<pre><code>scp ~/certs/pk-X509.pem ~/certs/cert-X509.pem your-user@your-servers-public-dns:/tmp/cert/
</code></pre>

<p>This will securely transfer your certs up to the server. Make sure to replace <code>your-user</code> with whatever user on OpsWorks you have permission to access, and <code>your-servers-public-dns</code> with the public DNS record for your server.</p>

<h2>3. Download the EC2 AMI tools</h2>

<p>I&rsquo;m not totally sure why the AMI tools and API tools are separate packages, but since they are you&rsquo;ll need to install them individually. For the AMI tools:</p>

<pre><code class="bash">sudo apt-get install -y unzip
wget http://s3.amazonaws.com/ec2-downloads/ec2-ami-tools.zip
sudo mkdir -p /usr/local/ec2
sudo unzip ec2-ami-tools.zip -d /usr/local/ec2
export EC2_AMITOOL_HOME=/usr/local/ec2/ec2-ami-tools-1.5.3/
export PATH=$EC2_AMITOOL_HOME/bin:$PATH
</code></pre>

<p>If you got a different version of the AMI tools than 1.5.3, you&rsquo;ll want to replace the AMI tools directory with the proper version.</p>

<h2>4. Download the EC2 API tools</h2>

<p>A similar process to step 3.</p>

<pre><code class="bash">wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip
sudo unzip ec2-api-tools.zip -d /usr/local/ec2
sudo apt-get install -y openjdk-7-jre
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre/
export EC2_HOME=/usr/local/ec2/ec2-api-tools-1.6.13.0/
export PATH=$PATH:$EC2_HOME/bin
export AWS_ACCESS_KEY=YourAccessKey
export AWS_SECRET_KEY=YourSecretKey
</code></pre>

<p>Again, if you downloaded a different version of the API tools, you&rsquo;ll need to change the API tools directory. Also replace <code>YourAccessKey</code> and <code>YourSecretKey</code> with your real access and secret keys.</p>

<h2>5. Ensure your version of GRUB is correct</h2>

<p>On Ubuntu 12.04, I didn&rsquo;t have to do anything except this:</p>

<pre><code class="bash">sudo apt-get install -y grub gdisk kpartx
</code></pre>

<p>But if your image has boot problems GRUB is the most likely culprit. Amazon has a <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-instance-store.html">good walkthrough</a> of how to set up legacy GRUB properly &ndash; following it should correct any boot issues you experience on your AMI.</p>

<h2>6. Stop all services</h2>

<p>Make sure everything and anything is stopped on the target server. A non-exclusive list:</p>

<pre><code class="bash">sudo service monit stop
sudo service mysql stop
sudo service nginx stop
sudo service redis stop
sudo service memcached stop
sudo service opsworks-agent stop
</code></pre>

<p>Running services can destroy the integrity of the image. Make sure everything is stopped before you waste your time!</p>

<h2>7. Remove instance configuration directories</h2>

<p>All of the instance-specific config directories must be destroyed, or OpsWorks will fail to provision the new image.</p>

<pre><code class="bash">sudo rm -rf /etc/aws/opsworks/ \
            /opt/aws/opsworks/ \
            /var/log/aws/opsworks/ \
            /var/lib/aws/opsworks/ \
            /etc/monit.d/opsworks-agent.monitrc \
            /etc/monit/conf.d/opsworks-agent.monitrc \
            /var/lib/cloud/
</code></pre>

<h2>8. Bundle the volume</h2>

<p>Finally, after all that setup, you&rsquo;re ready to actually bundle the volume.</p>

<pre><code class="bash">ec2-bundle-vol -k /tmp/cert/pk-X509.pem \
               -c /tmp/cert/cert-X509.pem \
               -u 123456789012 \
               -r x86_64 \
               -e /tmp/cert \
               -i $(find /etc /usr /opt -name '*.pem' -o -name '*.crt' -o -name '*.gpg' | tr '\n' ',')
</code></pre>

<p>Note that we provide the certificate locations as part of this command, so if your certs are named differently change that name above. Also you need to provide the account ID number for the -u flag. You can find this on your <a href="https://console.aws.amazon.com/iam/home?#security_credential">security credentials IAM page</a>, or if you need more help, check out Amazon&rsquo;s <a href="http://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html">documentation on finding your account ID number</a>.</p>

<p>This command will probably take a long while to run.</p>

<h2>9. Create and upload the volume to an S3 bucket</h2>

<p>Once the volume is bundled, go to S3 and create a bucket to receive the machine image. Then run this command on your instance:</p>

<pre><code class="bash">ec2-upload-bundle -b bucket_name/image_name \
                  -m /tmp/image.manifest.xml \
                  --region us-east-1
</code></pre>

<p>Replace <code>bucket_name</code> and <code>image_name</code> with the bucket you created in S3 and whatever you&rsquo;d like to name the image, and the region with whatever region your bucket is located in (and where you want the AMI to be registered). This will also take awhile to run.</p>

<h2>10. Register the AMI</h2>

<p>Only one step left, and this is an easy one! You can register the AMI with this command:</p>

<pre><code class="bash">ec2-register bucket_name/image_name/image.manifest.xml -n image_name --region us-east-1
</code></pre>

<p>You should now successfully see your new image in your list of registered AMIs for your region. Change your layer settings to use a custom image and select the AMI as the image for a new instance and try it out. Hopefully you&rsquo;ll have just cut out a fair amount of time from your instance provisioning process.</p>
]]></content>
  </entry>
  
</feed>
